{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"https://auto.gluon.ai/stable/tutorials/tabular/tabular-indepth.html","metadata":{}},{"cell_type":"code","source":"# Need to do this for each autogluon notebook...\n!pip install autogluon","metadata":{"execution":{"iopub.status.busy":"2024-07-06T05:57:43.172613Z","iopub.execute_input":"2024-07-06T05:57:43.173075Z","iopub.status.idle":"2024-07-06T06:02:00.339829Z","shell.execute_reply.started":"2024-07-06T05:57:43.173023Z","shell.execute_reply":"2024-07-06T06:02:00.338472Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting autogluon\n  Downloading autogluon-1.1.1-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.core==1.1.1 (from autogluon.core[all]==1.1.1->autogluon)\n  Downloading autogluon.core-1.1.1-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.features==1.1.1 (from autogluon)\n  Downloading autogluon.features-1.1.1-py3-none-any.whl.metadata (11 kB)\nCollecting autogluon.tabular==1.1.1 (from autogluon.tabular[all]==1.1.1->autogluon)\n  Downloading autogluon.tabular-1.1.1-py3-none-any.whl.metadata (13 kB)\nCollecting autogluon.multimodal==1.1.1 (from autogluon)\n  Downloading autogluon.multimodal-1.1.1-py3-none-any.whl.metadata (12 kB)\nCollecting autogluon.timeseries==1.1.1 (from autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading autogluon.timeseries-1.1.1-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: numpy<1.29,>=1.21 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.26.4)\nRequirement already satisfied: scipy<1.13,>=1.5.4 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.11.4)\nCollecting scikit-learn<1.4.1,>=1.3.0 (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: networkx<4,>=3.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.2.1)\nRequirement already satisfied: pandas<2.3.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.2.2)\nRequirement already satisfied: tqdm<5,>=4.38 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.32.3)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.7.5)\nRequirement already satisfied: boto3<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.26.100)\nCollecting autogluon.common==1.1.1 (from autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n  Downloading autogluon.common-1.1.1-py3-none-any.whl.metadata (11 kB)\nCollecting ray<2.11,>=2.10.0 (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\nRequirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /opt/conda/lib/python3.10/site-packages (from autogluon.core[all]==1.1.1->autogluon) (0.2.7)\nCollecting Pillow<11,>=10.0.1 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\nCollecting torch<2.4,>=2.2 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nCollecting lightning<2.4,>=2.2 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading lightning-2.3.2-py3-none-any.whl.metadata (54 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting transformers<4.41.0,>=4.38.0 (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting accelerate<0.22.0,>=0.21.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading accelerate-0.21.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: jsonschema<4.22,>=4.18 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (4.20.0)\nCollecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nCollecting timm<0.10.0,>=0.9.5 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading timm-0.9.16-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: torchvision<0.19.0,>=0.16.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (0.16.2+cpu)\nCollecting scikit-image<0.21.0,>=0.19.1 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nRequirement already satisfied: text-unidecode<1.4,>=1.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (1.3)\nCollecting torchmetrics<1.3.0,>=1.2.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading torchmetrics-1.2.1-py3-none-any.whl.metadata (20 kB)\nCollecting nptyping<2.5.0,>=1.4.4 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading nptyping-2.4.1-py3-none-any.whl.metadata (7.7 kB)\nCollecting omegaconf<2.3.0,>=2.1.1 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading omegaconf-2.2.3-py3-none-any.whl.metadata (3.9 kB)\nCollecting pytorch-metric-learning<2.4,>=1.3.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading pytorch_metric_learning-2.3.0-py3-none-any.whl.metadata (17 kB)\nCollecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\nCollecting nltk<4.0.0,>=3.4.5 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nCollecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (0.7.1)\nRequirement already satisfied: jinja2<3.2,>=3.0.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (3.1.2)\nRequirement already satisfied: tensorboard<3,>=2.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (2.15.1)\nRequirement already satisfied: pytesseract<0.3.11,>=0.3.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (0.3.10)\nCollecting nvidia-ml-py3==7.352.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: pdf2image<1.19,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from autogluon.multimodal==1.1.1->autogluon) (1.17.0)\nRequirement already satisfied: xgboost<2.1,>=1.6 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.0.3)\nRequirement already satisfied: fastai<2.8,>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.1.1->autogluon) (2.7.15)\nRequirement already satisfied: lightgbm<4.4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.1.1->autogluon) (4.2.0)\nRequirement already satisfied: catboost<1.3,>=1.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.tabular[all]==1.1.1->autogluon) (1.2.5)\nRequirement already satisfied: joblib<2,>=1.1 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (1.4.2)\nRequirement already satisfied: pytorch-lightning<2.4,>=2.2 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.2.5)\nCollecting gluonts==0.15.1 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading gluonts-0.15.1-py3-none-any.whl.metadata (9.9 kB)\nCollecting statsforecast<1.5,>=1.4.0 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading statsforecast-1.4.0-py3-none-any.whl.metadata (19 kB)\nCollecting mlforecast<0.10.1,>=0.10.0 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading mlforecast-0.10.0-py3-none-any.whl.metadata (11 kB)\nCollecting utilsforecast<0.0.11,>=0.0.10 (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading utilsforecast-0.0.10-py3-none-any.whl.metadata (7.0 kB)\nRequirement already satisfied: orjson~=3.9 in /opt/conda/lib/python3.10/site-packages (from autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (3.9.10)\nCollecting optimum<1.19,>=1.17 (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading optimum-1.18.1-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: psutil<6,>=5.7.3 in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (5.9.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (69.0.3)\nRequirement already satisfied: pydantic<3,>=1.7 in /opt/conda/lib/python3.10/site-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.5.3)\nRequirement already satisfied: toolz~=0.10 in /opt/conda/lib/python3.10/site-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.12.1)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (4.9.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (21.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon) (6.0.1)\nCollecting botocore<1.30.0,>=1.29.100 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon)\n  Downloading botocore-1.29.165-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.0.1)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (0.6.2)\nRequirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (0.20.3)\nRequirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (5.18.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (1.16.0)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (2.19.2)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.3.8)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (2024.3.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.23.2)\nRequirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (23.3.2)\nRequirement already satisfied: fastdownload<2,>=0.0.5 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.0.7)\nRequirement already satisfied: fastcore<1.6,>=1.5.29 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.5.43)\nRequirement already satisfied: fastprogress>=0.2.4 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.3)\nRequirement already satisfied: spacy<4 in /opt/conda/lib/python3.10/site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.7.4)\nRequirement already satisfied: future in /opt/conda/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (1.0.0)\nRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (2.2.1)\nRequirement already satisfied: py4j in /opt/conda/lib/python3.10/site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.1.1->autogluon) (0.10.9.7)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.1.1->autogluon) (2.1.3)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<4.22,>=4.18->autogluon.multimodal==1.1.1->autogluon) (0.16.2)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from lightning<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon) (0.11.2)\nRequirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.58.1)\nCollecting window-ops (from mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\nCollecting gdown>=4.0.0 (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon)\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (8.1.7)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==1.1.1->autogluon) (2023.12.25)\nCollecting antlr4-python3-runtime==4.9.* (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==1.1.1->autogluon)\n  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.4.6)\nCollecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\nCollecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (13.7.0)\nRequirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.9.0)\nCollecting coloredlogs (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.12.1)\nCollecting transformers<4.41.0,>=4.38.0 (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: onnx in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.16.1)\nCollecting onnxruntime>=1.11.0 (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: protobuf>=3.20.1 in /opt/conda/lib/python3.10/site-packages (from optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (3.20.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2023.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.13.1)\nRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.0.7)\nRequirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.3.1)\nRequirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.4.1)\nRequirement already satisfied: tensorboardX>=1.9 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.6.2.2)\nRequirement already satisfied: pyarrow>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (16.1.0)\nRequirement already satisfied: aiohttp>=3.7 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.9.1)\nCollecting aiohttp-cors (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n  Downloading aiohttp_cors-0.7.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: colorful in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.5.6)\nRequirement already satisfied: py-spy>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.3.14)\nRequirement already satisfied: opencensus in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.11.4)\nRequirement already satisfied: prometheus-client>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.19.0)\nRequirement already satisfied: smart-open in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (6.4.0)\nRequirement already satisfied: virtualenv!=20.21.1,>=20.0.24 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (20.21.0)\nRequirement already satisfied: grpcio>=1.42.0 in /opt/conda/lib/python3.10/site-packages (from ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.60.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (2024.2.2)\nRequirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2.33.1)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (2023.12.9)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (1.5.0)\nRequirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from scikit-image<0.21.0,>=0.19.1->autogluon.multimodal==1.1.1->autogluon) (0.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.2.0)\nRequirement already satisfied: statsmodels>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.14.1)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.4.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.5.2)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.0.3)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from timm<0.10.0,>=0.9.5->autogluon.multimodal==1.1.1->autogluon) (0.4.3)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.1 (from torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch<2.4,>=2.2->autogluon.multimodal==1.1.1->autogluon)\n  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nINFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\nCollecting torchvision<0.19.0,>=0.16.0 (from autogluon.multimodal==1.1.1->autogluon)\n  Downloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\nCollecting tokenizers<0.19,>=0.14 (from transformers<4.41.0,>=4.38.0->transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon)\n  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]<4.41.0,>=4.38.0->autogluon.multimodal==1.1.1->autogluon) (0.2.0)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->autogluon.core==1.1.1->autogluon.core[all]==1.1.1->autogluon) (3.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.9.3)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp>=3.7->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (4.0.3)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.1.1->autogluon) (0.6)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (4.12.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (1.3.1)\nRequirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->mlforecast<0.10.1,>=0.10.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.41.1)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (23.5.26)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.7->gluonts==0.15.1->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (2.14.6)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (8.2.3)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (2.0.10)\nRequirement already satisfied: weasel<0.4.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.3.4)\nRequirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.9.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (3.4.0)\nRequirement already satisfied: patsy>=0.5.4 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.13.2->statsforecast<1.5,>=1.4.0->autogluon.timeseries==1.1.1->autogluon.timeseries[all]==1.1.1->autogluon) (0.5.6)\nRequirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.3.8)\nRequirement already satisfied: platformdirs<4,>=2.4 in /opt/conda/lib/python3.10/site-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (3.11.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: ordered-set in /opt/conda/lib/python3.10/site-packages (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (4.1.0)\nRequirement already satisfied: opencensus-context>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (0.1.3)\nRequirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (2.11.1)\nRequirement already satisfied: pycryptodome in /opt/conda/lib/python3.10/site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.20.0)\nCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n  Downloading openxlab-0.1.1-py3-none-any.whl.metadata (3.8 kB)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==1.1.1->autogluon) (8.2.3)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (2.17.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum<1.19,>=1.17->optimum[onnxruntime]<1.19,>=1.17; extra == \"all\"->autogluon.timeseries[all]==1.1.1->autogluon) (1.3.0)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon) (1.62.0)\nRequirement already satisfied: language-data>=1.2 in /opt/conda/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.2.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon) (0.1.2)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==1.1.1->autogluon) (3.2.2)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.1.5)\nRequirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from weasel<0.4.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (0.16.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (2.5)\nCollecting filelock (from ray<2.11,>=2.10.0->ray[default,tune]<2.11,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.1.1->autogluon)\n  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\nCollecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n  Downloading oss2-2.17.0.tar.gz (259 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting packaging>=20.0 (from accelerate<0.22.0,>=0.21.0->autogluon.multimodal==1.1.1->autogluon)\n  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\nINFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\nCollecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.1.1->autogluon)\n  Downloading openxlab-0.1.0-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.36-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.35-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.34-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.33-py3-none-any.whl.metadata (3.8 kB)\nINFO: pip is still looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n  Downloading openxlab-0.0.32-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.31-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.30-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.29-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\nINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n  Downloading openxlab-0.0.27-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.26-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.25-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.24-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.23-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.22-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.21-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.20-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.19-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.18-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.17-py3-none-any.whl.metadata (3.7 kB)\n  Downloading openxlab-0.0.16-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.15-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.14-py3-none-any.whl.metadata (3.8 kB)\n  Downloading openxlab-0.0.13-py3-none-any.whl.metadata (4.5 kB)\n  Downloading openxlab-0.0.12-py3-none-any.whl.metadata (4.5 kB)\n  Downloading openxlab-0.0.11-py3-none-any.whl.metadata (4.3 kB)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.1.1->autogluon) (1.7.1)\nRequirement already satisfied: marisa-trie>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==1.1.1->autogluon) (1.1.1)\nDownloading autogluon-1.1.1-py3-none-any.whl (9.7 kB)\nDownloading autogluon.core-1.1.1-py3-none-any.whl (234 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.features-1.1.1-py3-none-any.whl (63 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.multimodal-1.1.1-py3-none-any.whl (427 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.0/428.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.tabular-1.1.1-py3-none-any.whl (312 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.timeseries-1.1.1-py3-none-any.whl (148 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.2/148.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading autogluon.common-1.1.1-py3-none-any.whl (64 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gluonts-0.15.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.21.0-py3-none-any.whl (244 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading lightning-2.3.2-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading mlforecast-0.10.0-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nptyping-2.4.1-py3-none-any.whl (36 kB)\nDownloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading optimum-1.18.1-py3-none-any.whl (410 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.0/410.0 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pillow-10.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pytorch_metric_learning-2.3.0-py3-none-any.whl (115 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_image-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading statsforecast-1.4.0-py3-none-any.whl (91 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading timm-0.9.16-py3-none-any.whl (2.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading torchvision-0.18.1-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading utilsforecast-0.0.10-py3-none-any.whl (30 kB)\nDownloading botocore-1.29.165-py3-none-any.whl (11.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nDownloading onnxruntime-1.18.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading model_index-0.1.11-py3-none-any.whl (34 kB)\nDownloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\nDownloading window_ops-0.0.15-py3-none-any.whl (15 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading openxlab-0.0.11-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3, antlr4-python3-runtime, seqeval\n  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19171 sha256=312c1206de2c8c320b06323e59468f91f32fe850a5d12e6196e5d1a39087a337\n  Stored in directory: /root/.cache/pip/wheels/5c/d8/c0/46899f8be7a75a2ffd197a23c8797700ea858b9b34819fbf9e\n  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=f962fb04349c527972aebea1aa34185237929407ffad0bc5f5f27e348e6c492c\n  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=51570a7a8a926ba5f207c6c839b4e42103f63d3fb8486f6bc7f8bfd63d8f6430\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built nvidia-ml-py3 antlr4-python3-runtime seqeval\nInstalling collected packages: nvidia-ml-py3, antlr4-python3-runtime, triton, Pillow, openxlab, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nptyping, nltk, model-index, humanfriendly, window-ops, scikit-learn, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, botocore, utilsforecast, tokenizers, seqeval, scikit-image, opendatalab, onnxruntime, nvidia-cusolver-cu12, gluonts, gdown, aiohttp-cors, transformers, torch, statsforecast, ray, openmim, nlpaug, mlforecast, torchvision, torchmetrics, pytorch-metric-learning, evaluate, autogluon.common, accelerate, timm, optimum, autogluon.features, autogluon.core, lightning, autogluon.tabular, autogluon.multimodal, autogluon.timeseries, autogluon\n  Attempting uninstall: Pillow\n    Found existing installation: Pillow 9.5.0\n    Uninstalling Pillow-9.5.0:\n      Successfully uninstalled Pillow-9.5.0\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.34.106\n    Uninstalling botocore-1.34.106:\n      Successfully uninstalled botocore-1.34.106\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.19.1\n    Uninstalling tokenizers-0.19.1:\n      Successfully uninstalled tokenizers-0.19.1\n  Attempting uninstall: scikit-image\n    Found existing installation: scikit-image 0.22.0\n    Uninstalling scikit-image-0.22.0:\n      Successfully uninstalled scikit-image-0.22.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\n  Attempting uninstall: torch\n    Found existing installation: torch 2.1.2+cpu\n    Uninstalling torch-2.1.2+cpu:\n      Successfully uninstalled torch-2.1.2+cpu\n  Attempting uninstall: ray\n    Found existing installation: ray 2.9.0\n    Uninstalling ray-2.9.0:\n      Successfully uninstalled ray-2.9.0\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.16.2+cpu\n    Uninstalling torchvision-0.16.2+cpu:\n      Successfully uninstalled torchvision-0.16.2+cpu\n  Attempting uninstall: torchmetrics\n    Found existing installation: torchmetrics 1.4.0.post0\n    Uninstalling torchmetrics-1.4.0.post0:\n      Successfully uninstalled torchmetrics-1.4.0.post0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\n  Attempting uninstall: timm\n    Found existing installation: timm 1.0.3\n    Uninstalling timm-1.0.3:\n      Successfully uninstalled timm-1.0.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.13.0 requires aiohttp<4.0.0,>=3.9.2, but you have aiohttp 3.9.1 which is incompatible.\naiobotocore 2.13.0 requires botocore<1.34.107,>=1.34.70, but you have botocore 1.29.165 which is incompatible.\nalbumentations 1.4.0 requires scikit-image>=0.21.0, but you have scikit-image 0.20.0 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntorchaudio 2.1.2+cpu requires torch==2.1.2, but you have torch 2.3.1 which is incompatible.\ntorchtext 0.16.2+cpu requires torch==2.1.2, but you have torch 2.3.1 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed Pillow-10.3.0 accelerate-0.21.0 aiohttp-cors-0.7.0 antlr4-python3-runtime-4.9.3 autogluon-1.1.1 autogluon.common-1.1.1 autogluon.core-1.1.1 autogluon.features-1.1.1 autogluon.multimodal-1.1.1 autogluon.tabular-1.1.1 autogluon.timeseries-1.1.1 botocore-1.29.165 coloredlogs-15.0.1 evaluate-0.4.2 gdown-5.2.0 gluonts-0.15.1 humanfriendly-10.0 lightning-2.3.2 mlforecast-0.10.0 model-index-0.1.11 nlpaug-1.1.11 nltk-3.8.1 nptyping-2.4.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-ml-py3-7.352.0 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 omegaconf-2.2.3 onnxruntime-1.18.1 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 optimum-1.18.1 pytorch-metric-learning-2.3.0 ray-2.10.0 scikit-image-0.20.0 scikit-learn-1.4.0 seqeval-1.2.2 statsforecast-1.4.0 timm-0.9.16 tokenizers-0.15.2 torch-2.3.1 torchmetrics-1.2.1 torchvision-0.18.1 transformers-4.39.3 triton-2.3.1 utilsforecast-0.0.10 window-ops-0.0.15\n","output_type":"stream"}]},{"cell_type":"code","source":"from autogluon.tabular import TabularDataset, TabularPredictor\n\nimport numpy as np\n\ntrain_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv')\nsubsample_size = 1000  # subsample subset of data for faster demo, try setting this to much larger values\ntrain_data = train_data.sample(n=subsample_size, random_state=0)\nprint(train_data.head())\n\nlabel = 'occupation'\nprint(\"Summary of occupation column: \\n\", train_data['occupation'].describe())\n\ntest_data = TabularDataset('https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv')\ny_test = test_data[label]\ntest_data_nolabel = test_data.drop(columns=[label])  # delete label column\n\nmetric = 'accuracy' # we specify eval-metric just for demo (unnecessary as it's the default)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:02:00.342341Z","iopub.execute_input":"2024-07-06T06:02:00.342716Z","iopub.status.idle":"2024-07-06T06:02:05.320246Z","shell.execute_reply.started":"2024-07-06T06:02:00.342680Z","shell.execute_reply":"2024-07-06T06:02:05.318855Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/train.csv | Columns = 15 / 15 | Rows = 39073 -> 39073\n","output_type":"stream"},{"name":"stdout","text":"       age workclass  fnlwgt      education  education-num  \\\n6118    51   Private   39264   Some-college             10   \n23204   58   Private   51662           10th              6   \n29590   40   Private  326310   Some-college             10   \n18116   37   Private  222450        HS-grad              9   \n33964   62   Private  109190      Bachelors             13   \n\n            marital-status        occupation    relationship    race      sex  \\\n6118    Married-civ-spouse   Exec-managerial            Wife   White   Female   \n23204   Married-civ-spouse     Other-service            Wife   White   Female   \n29590   Married-civ-spouse      Craft-repair         Husband   White     Male   \n18116        Never-married             Sales   Not-in-family   White     Male   \n33964   Married-civ-spouse   Exec-managerial         Husband   White     Male   \n\n       capital-gain  capital-loss  hours-per-week  native-country   class  \n6118              0             0              40   United-States    >50K  \n23204             0             0               8   United-States   <=50K  \n29590             0             0              44   United-States   <=50K  \n18116             0          2339              40     El-Salvador   <=50K  \n33964         15024             0              40   United-States    >50K  \nSummary of occupation column: \n count              1000\nunique               15\ntop        Craft-repair\nfreq                142\nName: occupation, dtype: object\n","output_type":"stream"},{"name":"stderr","text":"Loaded data from: https://autogluon.s3.amazonaws.com/datasets/Inc/test.csv | Columns = 15 / 15 | Rows = 9769 -> 9769\n","output_type":"stream"}]},{"cell_type":"code","source":"from autogluon.common import space\n\nnn_options = {  # specifies non-default hyperparameter values for neural network models\n    'num_epochs': 10,  # number of training epochs (controls training time of NN models)\n    'learning_rate': space.Real(1e-4, 1e-2, default=5e-4, log=True),  # learning rate used in training (real-valued hyperparameter searched on log-scale)\n    'activation': space.Categorical('relu', 'softrelu', 'tanh'),  # activation function used in NN (categorical hyperparameter, default = first entry)\n    'dropout_prob': space.Real(0.0, 0.5, default=0.1),  # dropout probability (real-valued hyperparameter)\n}\n\ngbm_options = {  # specifies non-default hyperparameter values for lightGBM gradient boosted trees\n    'num_boost_round': 100,  # number of boosting rounds (controls training time of GBM models)\n    'num_leaves': space.Int(lower=26, upper=66, default=36),  # number of leaves in trees (integer hyperparameter)\n}\n\nhyperparameters = {  # hyperparameters of each model type\n                   'GBM': gbm_options,\n                   'NN_TORCH': nn_options,  # NOTE: comment this line out if you get errors on Mac OSX\n                  }  # When these keys are missing from hyperparameters dict, no models of that type are trained\n\ntime_limit = 2*60  # train various models for ~2 min\nnum_trials = 5  # try at most 5 different hyperparameter configurations for each type of model\nsearch_strategy = 'auto'  # to tune hyperparameters using random search routine with a local scheduler\n\nhyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n    'num_trials': num_trials,\n    'scheduler' : 'local',\n    'searcher': search_strategy,\n}  # Refer to TabularPredictor.fit docstring for all valid values\n\npredictor = TabularPredictor(label=label, eval_metric=metric).fit(\n    train_data,\n    time_limit=time_limit,\n    hyperparameters=hyperparameters,\n    hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:02:05.321713Z","iopub.execute_input":"2024-07-06T06:02:05.322150Z","iopub.status.idle":"2024-07-06T06:02:49.589751Z","shell.execute_reply.started":"2024-07-06T06:02:05.322117Z","shell.execute_reply":"2024-07-06T06:02:49.588459Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"Fitted model: NeuralNetTorch/7d0f6800 ...\n\t0.355\t = Validation score   (accuracy)\n\t6.27s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitted model: NeuralNetTorch/c54b2fa6 ...\n\t0.31\t = Validation score   (accuracy)\n\t7.43s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitted model: NeuralNetTorch/33d46cfd ...\n\t0.335\t = Validation score   (accuracy)\n\t9.3s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitted model: NeuralNetTorch/cb80801b ...\n\t0.31\t = Validation score   (accuracy)\n\t6.19s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitted model: NeuralNetTorch/53655f5c ...\n\t0.29\t = Validation score   (accuracy)\n\t5.75s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 119.76s of the 76.46s of remaining time.\n\tEnsemble Weights: {'LightGBM/T3': 1.0}\n\t0.375\t = Validation score   (accuracy)\n\t0.19s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 43.82s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 20222.3 rows/s (200 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240706_060205\")\n","output_type":"stream"}]},{"cell_type":"code","source":"y_pred = predictor.predict(test_data_nolabel)\nprint(\"Predictions:  \", list(y_pred)[:5])\nperf = predictor.evaluate(test_data, auxiliary_metrics=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:02:49.592614Z","iopub.execute_input":"2024-07-06T06:02:49.593632Z","iopub.status.idle":"2024-07-06T06:02:50.194876Z","shell.execute_reply.started":"2024-07-06T06:02:49.593590Z","shell.execute_reply":"2024-07-06T06:02:50.193526Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Predictions:   [' Other-service', ' Craft-repair', ' Exec-managerial', ' Sales', ' Other-service']\n","output_type":"stream"}]},{"cell_type":"code","source":"results = predictor.fit_summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:02:50.196575Z","iopub.execute_input":"2024-07-06T06:02:50.197042Z","iopub.status.idle":"2024-07-06T06:02:51.040017Z","shell.execute_reply.started":"2024-07-06T06:02:50.196999Z","shell.execute_reply":"2024-07-06T06:02:51.038709Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"*** Summary of fit() ***\nEstimated performance of each model:\n                      model  score_val eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0               LightGBM/T3      0.375    accuracy       0.008328  0.643557                0.008328           0.643557            1       True          3\n1       WeightedEnsemble_L2      0.375    accuracy       0.009890  0.831291                0.001562           0.187734            2       True         11\n2               LightGBM/T5      0.375    accuracy       0.011104  0.942165                0.011104           0.942165            1       True          5\n3               LightGBM/T1      0.370    accuracy       0.006780  2.427496                0.006780           2.427496            1       True          1\n4               LightGBM/T4      0.360    accuracy       0.015970  1.047170                0.015970           1.047170            1       True          4\n5               LightGBM/T2      0.355    accuracy       0.009725  1.095921                0.009725           1.095921            1       True          2\n6   NeuralNetTorch/7d0f6800      0.355    accuracy       0.018363  6.272576                0.018363           6.272576            1       True          6\n7   NeuralNetTorch/33d46cfd      0.335    accuracy       0.023813  9.300223                0.023813           9.300223            1       True          8\n8   NeuralNetTorch/cb80801b      0.310    accuracy       0.019670  6.187877                0.019670           6.187877            1       True          9\n9   NeuralNetTorch/c54b2fa6      0.310    accuracy       0.021230  7.429606                0.021230           7.429606            1       True          7\n10  NeuralNetTorch/53655f5c      0.290    accuracy       0.017289  5.745520                0.017289           5.745520            1       True         10\nNumber of models trained: 11\nTypes of models trained:\n{'WeightedEnsembleModel', 'TabularNeuralNetTorchModel', 'LGBModel'}\nBagging used: False \nMulti-layer stack-ensembling used: False \nFeature Metadata (Processed):\n(raw dtype, special dtypes):\n('category', [])  : 6 | ['workclass', 'education', 'marital-status', 'relationship', 'race', ...]\n('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n('int', ['bool']) : 2 | ['sex', 'class']\nPlot summary of models saved to file: AutogluonModels/ag-20240706_060205SummaryOfModels.html\n*** End of fit() summary ***\n","output_type":"stream"}]},{"cell_type":"code","source":"label = 'class'  # Now lets predict the \"class\" column (binary classification)\ntest_data_nolabel = test_data.drop(columns=[label])\ny_test = test_data[label]\nsave_path = 'agModels-predictClass'  # folder where to store trained models\n\npredictor = TabularPredictor(label=label, eval_metric=metric).fit(train_data,\n    num_bag_folds=5, num_bag_sets=1, num_stack_levels=1,\n    hyperparameters = {'NN_TORCH': {'num_epochs': 2}, 'GBM': {'num_boost_round': 20}},  # last  argument is just for quick demo here, omit it in real applications\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:02:51.041567Z","iopub.execute_input":"2024-07-06T06:02:51.042017Z","iopub.status.idle":"2024-07-06T06:03:49.442628Z","shell.execute_reply.started":"2024-07-06T06:02:51.041967Z","shell.execute_reply":"2024-07-06T06:03:49.441028Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20240706_060251\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\nCPU Count:          4\nMemory Avail:       29.45 GB / 31.36 GB (93.9%)\nDisk Space Avail:   19.48 GB / 19.52 GB (99.8%)\n===================================================\nNo presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\nBeginning AutoGluon training ...\nAutoGluon will save models to \"AutogluonModels/ag-20240706_060251\"\nTrain Data Rows:    1000\nTrain Data Columns: 14\nLabel Column:       class\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\t2 unique label values:  [' >50K', ' <=50K']\n\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    30156.40 MB\n\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n\t\t('int', ['bool']) : 1 | ['sex']\n\t0.1s = Fit runtime\n\t14 features in original data used to generate 14 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.19s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\tTo change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': {'num_epochs': 2},\n\t'GBM': {'num_boost_round': 20},\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nFitting 2 L1 models ...\nFitting model: LightGBM_BAG_L1 ...\n\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.01%)\n\t0.823\t = Validation score   (accuracy)\n\t6.71s\t = Training   runtime\n\t0.04s\t = Validation runtime\nFitting model: NeuralNetTorch_BAG_L1 ...\n\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.00%)\n\t0.744\t = Validation score   (accuracy)\n\t13.2s\t = Training   runtime\n\t0.13s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ...\n\tEnsemble Weights: {'LightGBM_BAG_L1': 1.0}\n\t0.823\t = Validation score   (accuracy)\n\t0.05s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting 2 L2 models ...\nFitting model: LightGBM_BAG_L2 ...\n\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.01%)\n\t0.828\t = Validation score   (accuracy)\n\t6.65s\t = Training   runtime\n\t0.1s\t = Validation runtime\nFitting model: NeuralNetTorch_BAG_L2 ...\n\tFitting 5 child models (S1F1 - S1F5) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.00%)\n\t0.748\t = Validation score   (accuracy)\n\t14.89s\t = Training   runtime\n\t0.13s\t = Validation runtime\nFitting model: WeightedEnsemble_L3 ...\n\tEnsemble Weights: {'LightGBM_BAG_L2': 0.833, 'LightGBM_BAG_L1': 0.167}\n\t0.829\t = Validation score   (accuracy)\n\t0.1s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 58.34s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 738.5 rows/s (200 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240706_060251\")\n","output_type":"stream"}]},{"cell_type":"code","source":"# Lets also specify the \"f1\" metric\npredictor = TabularPredictor(label=label, eval_metric='f1', path=save_path).fit(\n    train_data, auto_stack=True,\n    time_limit=30, hyperparameters={'FASTAI': {'num_epochs': 10}, 'GBM': {'num_boost_round': 200}}  # last 2 arguments are for quick demo, omit them in real applications\n)\npredictor.leaderboard(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:03:49.446002Z","iopub.execute_input":"2024-07-06T06:03:49.446587Z","iopub.status.idle":"2024-07-06T06:04:48.820076Z","shell.execute_reply.started":"2024-07-06T06:03:49.446542Z","shell.execute_reply":"2024-07-06T06:04:48.818917Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"Verbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\nCPU Count:          4\nMemory Avail:       24.47 GB / 31.36 GB (78.0%)\nDisk Space Avail:   19.47 GB / 19.52 GB (99.8%)\n===================================================\nNo presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\nStack configuration (auto_stack=True): num_stack_levels=0, num_bag_folds=8, num_bag_sets=5\nBeginning AutoGluon training ... Time limit = 30s\nAutoGluon will save models to \"agModels-predictClass\"\nTrain Data Rows:    1000\nTrain Data Columns: 14\nLabel Column:       class\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\t2 unique label values:  [' >50K', ' <=50K']\n\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    24989.69 MB\n\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n\t\t('int', ['bool']) : 1 | ['sex']\n\t0.2s = Fit runtime\n\t14 features in original data used to generate 14 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.33s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'f1'\n\tTo change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n\t'FASTAI': {'num_epochs': 10},\n\t'GBM': {'num_boost_round': 200},\n}\nFitting 2 L1 models ...\nFitting model: LightGBM_BAG_L1 ... Training model for up to 29.67s of the 29.66s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.01%)\n\t0.6856\t = Validation score   (f1)\n\t15.99s\t = Training   runtime\n\t0.24s\t = Validation runtime\nFitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 6.24s of the 6.23s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.00%)\n\t0.6892\t = Validation score   (f1)\n\t26.16s\t = Training   runtime\n\t0.32s\t = Validation runtime\nCompleted 1/5 k-fold bagging repeats ...\nFitting model: WeightedEnsemble_L2 ... Training model for up to 29.67s of the -25.04s of remaining time.\n\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 1.0}\n\t0.6892\t = Validation score   (f1)\n\t0.2s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 55.29s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 394.1 rows/s (125 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"agModels-predictClass\")\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                    model  score_test  score_val eval_metric  pred_time_test  \\\n0  NeuralNetFastAI_BAG_L1    0.648383   0.689243          f1        3.358280   \n1     WeightedEnsemble_L2    0.648383   0.689243          f1        3.360578   \n2         LightGBM_BAG_L1    0.629437   0.685590          f1        0.563756   \n\n   pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0       0.316566  26.157116                 3.358280                0.316566   \n1       0.321463  26.357361                 0.002298                0.004896   \n2       0.238575  15.994304                 0.563756                0.238575   \n\n   fit_time_marginal  stack_level  can_infer  fit_order  \n0          26.157116            1       True          2  \n1           0.200245            2       True          3  \n2          15.994304            1       True          1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_test</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_test</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_test_marginal</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NeuralNetFastAI_BAG_L1</td>\n      <td>0.648383</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>3.358280</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>3.358280</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>0.648383</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>3.360578</td>\n      <td>0.321463</td>\n      <td>26.357361</td>\n      <td>0.002298</td>\n      <td>0.004896</td>\n      <td>0.200245</td>\n      <td>2</td>\n      <td>True</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LightGBM_BAG_L1</td>\n      <td>0.629437</td>\n      <td>0.685590</td>\n      <td>f1</td>\n      <td>0.563756</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>0.563756</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print(f'Prior to calibration (predictor.decision_threshold={predictor.decision_threshold}):')\nscores = predictor.evaluate(test_data)\n\ncalibrated_decision_threshold = predictor.calibrate_decision_threshold()\npredictor.set_decision_threshold(calibrated_decision_threshold)\n\nprint(f'After calibration (predictor.decision_threshold={predictor.decision_threshold}):')\nscores_calibrated = predictor.evaluate(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:04:48.821839Z","iopub.execute_input":"2024-07-06T06:04:48.823628Z","iopub.status.idle":"2024-07-06T06:04:52.973182Z","shell.execute_reply.started":"2024-07-06T06:04:48.823581Z","shell.execute_reply":"2024-07-06T06:04:52.972148Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Prior to calibration (predictor.decision_threshold=0.5):\n","output_type":"stream"},{"name":"stderr","text":"Calibrating decision threshold to optimize metric f1 | Checking 51 thresholds...\nCalibrating decision threshold via fine-grained search | Checking 38 thresholds...\n\tBase Threshold: 0.500\t| val: 0.6892\n\tBest Threshold: 0.500\t| val: 0.6892\n","output_type":"stream"},{"name":"stdout","text":"After calibration (predictor.decision_threshold=0.5):\n","output_type":"stream"}]},{"cell_type":"code","source":"for metric_name in scores:\n    metric_score = scores[metric_name]\n    metric_score_calibrated = scores_calibrated[metric_name]\n    decision_threshold = predictor.decision_threshold\n    print(f'decision_threshold={decision_threshold:.3f}\\t| metric=\"{metric_name}\"'\n          f'\\n\\ttest_score uncalibrated: {metric_score:.4f}'\n          f'\\n\\ttest_score   calibrated: {metric_score_calibrated:.4f}'\n          f'\\n\\ttest_score        delta: {metric_score_calibrated-metric_score:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:04:52.974619Z","iopub.execute_input":"2024-07-06T06:04:52.974988Z","iopub.status.idle":"2024-07-06T06:04:52.982131Z","shell.execute_reply.started":"2024-07-06T06:04:52.974957Z","shell.execute_reply":"2024-07-06T06:04:52.981020Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"decision_threshold=0.500\t| metric=\"f1\"\n\ttest_score uncalibrated: 0.6484\n\ttest_score   calibrated: 0.6484\n\ttest_score        delta: 0.0000\ndecision_threshold=0.500\t| metric=\"accuracy\"\n\ttest_score uncalibrated: 0.8465\n\ttest_score   calibrated: 0.8465\n\ttest_score        delta: 0.0000\ndecision_threshold=0.500\t| metric=\"balanced_accuracy\"\n\ttest_score uncalibrated: 0.7604\n\ttest_score   calibrated: 0.7604\n\ttest_score        delta: 0.0000\ndecision_threshold=0.500\t| metric=\"mcc\"\n\ttest_score uncalibrated: 0.5545\n\ttest_score   calibrated: 0.5545\n\ttest_score        delta: 0.0000\ndecision_threshold=0.500\t| metric=\"roc_auc\"\n\ttest_score uncalibrated: 0.8941\n\ttest_score   calibrated: 0.8941\n\ttest_score        delta: 0.0000\ndecision_threshold=0.500\t| metric=\"precision\"\n\ttest_score uncalibrated: 0.7100\n\ttest_score   calibrated: 0.7100\n\ttest_score        delta: 0.0000\ndecision_threshold=0.500\t| metric=\"recall\"\n\ttest_score uncalibrated: 0.5966\n\ttest_score   calibrated: 0.5966\n\ttest_score        delta: 0.0000\n","output_type":"stream"}]},{"cell_type":"code","source":"predictor.set_decision_threshold(0.5)  # Reset decision threshold\nfor metric_name in ['f1', 'balanced_accuracy', 'mcc']:\n    metric_score = predictor.evaluate(test_data, silent=True)[metric_name]\n    calibrated_decision_threshold = predictor.calibrate_decision_threshold(metric=metric_name, verbose=False)\n    metric_score_calibrated = predictor.evaluate(\n        test_data, decision_threshold=calibrated_decision_threshold, silent=True\n    )[metric_name]\n    print(f'decision_threshold={calibrated_decision_threshold:.3f}\\t| metric=\"{metric_name}\"'\n          f'\\n\\ttest_score uncalibrated: {metric_score:.4f}'\n          f'\\n\\ttest_score   calibrated: {metric_score_calibrated:.4f}'\n          f'\\n\\ttest_score        delta: {metric_score_calibrated-metric_score:.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:04:52.988198Z","iopub.execute_input":"2024-07-06T06:04:52.989277Z","iopub.status.idle":"2024-07-06T06:05:04.945276Z","shell.execute_reply.started":"2024-07-06T06:04:52.989244Z","shell.execute_reply":"2024-07-06T06:05:04.944167Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"decision_threshold=0.500\t| metric=\"f1\"\n\ttest_score uncalibrated: 0.6484\n\ttest_score   calibrated: 0.6484\n\ttest_score        delta: 0.0000\ndecision_threshold=0.484\t| metric=\"balanced_accuracy\"\n\ttest_score uncalibrated: 0.7604\n\ttest_score   calibrated: 0.7643\n\ttest_score        delta: 0.0039\ndecision_threshold=0.500\t| metric=\"mcc\"\n\ttest_score uncalibrated: 0.5545\n\ttest_score   calibrated: 0.5545\n\ttest_score        delta: 0.0000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"predictor = TabularPredictor.load(save_path)  # `predictor.path` is another way to get the relative path needed to later load predictor.","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:04.946999Z","iopub.execute_input":"2024-07-06T06:05:04.947670Z","iopub.status.idle":"2024-07-06T06:05:04.957171Z","shell.execute_reply.started":"2024-07-06T06:05:04.947638Z","shell.execute_reply":"2024-07-06T06:05:04.955871Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"predictor.features()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:04.958744Z","iopub.execute_input":"2024-07-06T06:05:04.959252Z","iopub.status.idle":"2024-07-06T06:05:04.970668Z","shell.execute_reply.started":"2024-07-06T06:05:04.959219Z","shell.execute_reply":"2024-07-06T06:05:04.969575Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"['age',\n 'workclass',\n 'fnlwgt',\n 'education',\n 'education-num',\n 'marital-status',\n 'occupation',\n 'relationship',\n 'race',\n 'sex',\n 'capital-gain',\n 'capital-loss',\n 'hours-per-week',\n 'native-country']"},"metadata":{}}]},{"cell_type":"code","source":"datapoint = test_data_nolabel.iloc[[0]]  # Note: .iloc[0] won't work because it returns pandas Series instead of DataFrame\nprint(datapoint)\npredictor.predict(datapoint)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:04.972400Z","iopub.execute_input":"2024-07-06T06:05:04.973219Z","iopub.status.idle":"2024-07-06T06:05:05.219176Z","shell.execute_reply.started":"2024-07-06T06:05:04.973179Z","shell.execute_reply":"2024-07-06T06:05:05.218200Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"   age workclass  fnlwgt education  education-num       marital-status  \\\n0   31   Private  169085      11th              7   Married-civ-spouse   \n\n  occupation relationship    race      sex  capital-gain  capital-loss  \\\n0      Sales         Wife   White   Female             0             0   \n\n   hours-per-week  native-country  \n0              20   United-States  \n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0     <=50K\nName: class, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"predictor.predict_proba(datapoint)  # returns a DataFrame that shows which probability corresponds to which class","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:05.220692Z","iopub.execute_input":"2024-07-06T06:05:05.221313Z","iopub.status.idle":"2024-07-06T06:05:05.447766Z","shell.execute_reply.started":"2024-07-06T06:05:05.221271Z","shell.execute_reply":"2024-07-06T06:05:05.446636Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"      <=50K      >50K\n0  0.850133  0.149867","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>&lt;=50K</th>\n      <th>&gt;50K</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.850133</td>\n      <td>0.149867</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"predictor.model_best","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:05.449347Z","iopub.execute_input":"2024-07-06T06:05:05.449726Z","iopub.status.idle":"2024-07-06T06:05:05.457447Z","shell.execute_reply.started":"2024-07-06T06:05:05.449692Z","shell.execute_reply":"2024-07-06T06:05:05.456095Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'WeightedEnsemble_L2'"},"metadata":{}}]},{"cell_type":"code","source":"predictor.leaderboard(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:05.458902Z","iopub.execute_input":"2024-07-06T06:05:05.459290Z","iopub.status.idle":"2024-07-06T06:05:07.837590Z","shell.execute_reply.started":"2024-07-06T06:05:05.459260Z","shell.execute_reply":"2024-07-06T06:05:07.836153Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                    model  score_test  score_val eval_metric  pred_time_test  \\\n0  NeuralNetFastAI_BAG_L1    0.648383   0.689243          f1        1.837715   \n1     WeightedEnsemble_L2    0.648383   0.689243          f1        1.841470   \n2         LightGBM_BAG_L1    0.629437   0.685590          f1        0.442313   \n\n   pred_time_val   fit_time  pred_time_test_marginal  pred_time_val_marginal  \\\n0       0.316566  26.157116                 1.837715                0.316566   \n1       0.321463  26.357361                 0.003755                0.004896   \n2       0.238575  15.994304                 0.442313                0.238575   \n\n   fit_time_marginal  stack_level  can_infer  fit_order  \n0          26.157116            1       True          2  \n1           0.200245            2       True          3  \n2          15.994304            1       True          1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_test</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_test</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_test_marginal</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NeuralNetFastAI_BAG_L1</td>\n      <td>0.648383</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>1.837715</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>1.837715</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>0.648383</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>1.841470</td>\n      <td>0.321463</td>\n      <td>26.357361</td>\n      <td>0.003755</td>\n      <td>0.004896</td>\n      <td>0.200245</td>\n      <td>2</td>\n      <td>True</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LightGBM_BAG_L1</td>\n      <td>0.629437</td>\n      <td>0.685590</td>\n      <td>f1</td>\n      <td>0.442313</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>0.442313</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"predictor.leaderboard(extra_info=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:07.839191Z","iopub.execute_input":"2024-07-06T06:05:07.839565Z","iopub.status.idle":"2024-07-06T06:05:13.010350Z","shell.execute_reply.started":"2024-07-06T06:05:07.839534Z","shell.execute_reply":"2024-07-06T06:05:13.009113Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                    model  score_val eval_metric  pred_time_val   fit_time  \\\n0  NeuralNetFastAI_BAG_L1   0.689243          f1       0.316566  26.157116   \n1     WeightedEnsemble_L2   0.689243          f1       0.321463  26.357361   \n2         LightGBM_BAG_L1   0.685590          f1       0.238575  15.994304   \n\n   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n0                0.316566          26.157116            1       True   \n1                0.004896           0.200245            2       True   \n2                0.238575          15.994304            1       True   \n\n   fit_order  ...  \\\n0          2  ...   \n1          3  ...   \n2          1  ...   \n\n                                                                                              hyperparameters  \\\n0   {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}   \n1  {'use_orig_features': False, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}   \n2   {'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}   \n\n   hyperparameters_fit  \\\n0                   {}   \n1                   {}   \n2                   {}   \n\n                                                                                                                                                                                                                                                                                                                                                                           ag_args_fit  \\\n0  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}   \n1  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}   \n2  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}   \n\n                                                                                                                                                              features  \\\n0  [fnlwgt, workclass, education-num, sex, race, occupation, capital-loss, hours-per-week, relationship, native-country, marital-status, education, capital-gain, age]   \n1                                                                                                                                             [NeuralNetFastAI_BAG_L1]   \n2  [fnlwgt, workclass, education-num, sex, race, occupation, capital-loss, hours-per-week, relationship, native-country, marital-status, education, capital-gain, age]   \n\n   compile_time  \\\n0          None   \n1          None   \n2          None   \n\n                                                                                                                                                                             child_hyperparameters  \\\n0  {'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0, 'num_epochs': 10}   \n1                                                                                                                                                 {'ensemble_size': 25, 'subsample_size': 1000000}   \n2                                                                                                                                                  {'learning_rate': 0.05, 'num_boost_round': 200}   \n\n          child_hyperparameters_fit  \\\n0  {'epochs': 30, 'best_epoch': 10}   \n1              {'ensemble_size': 1}   \n2           {'num_boost_round': 83}   \n\n                                                                                                                                                                                                                                                                                                                                                                                                             child_ag_args_fit  \\\n0  {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None}   \n1                                          {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}   \n2                                {'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None}   \n\n                  ancestors            descendants  \n0                        []  [WeightedEnsemble_L2]  \n1  [NeuralNetFastAI_BAG_L1]                     []  \n2                        []                     []  \n\n[3 rows x 32 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n      <th>...</th>\n      <th>hyperparameters</th>\n      <th>hyperparameters_fit</th>\n      <th>ag_args_fit</th>\n      <th>features</th>\n      <th>compile_time</th>\n      <th>child_hyperparameters</th>\n      <th>child_hyperparameters_fit</th>\n      <th>child_ag_args_fit</th>\n      <th>ancestors</th>\n      <th>descendants</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NeuralNetFastAI_BAG_L1</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n      <td>...</td>\n      <td>{'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}</td>\n      <td>{}</td>\n      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}</td>\n      <td>[fnlwgt, workclass, education-num, sex, race, occupation, capital-loss, hours-per-week, relationship, native-country, marital-status, education, capital-gain, age]</td>\n      <td>None</td>\n      <td>{'layers': None, 'emb_drop': 0.1, 'ps': 0.1, 'bs': 'auto', 'lr': 0.01, 'epochs': 'auto', 'early.stopping.min_delta': 0.0001, 'early.stopping.patience': 20, 'smoothing': 0.0, 'num_epochs': 10}</td>\n      <td>{'epochs': 30, 'best_epoch': 10}</td>\n      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': ['text_ngram', 'text_as_category'], 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None}</td>\n      <td>[]</td>\n      <td>[WeightedEnsemble_L2]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>0.321463</td>\n      <td>26.357361</td>\n      <td>0.004896</td>\n      <td>0.200245</td>\n      <td>2</td>\n      <td>True</td>\n      <td>3</td>\n      <td>...</td>\n      <td>{'use_orig_features': False, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}</td>\n      <td>{}</td>\n      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}</td>\n      <td>[NeuralNetFastAI_BAG_L1]</td>\n      <td>None</td>\n      <td>{'ensemble_size': 25, 'subsample_size': 1000000}</td>\n      <td>{'ensemble_size': 1}</td>\n      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}</td>\n      <td>[NeuralNetFastAI_BAG_L1]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LightGBM_BAG_L1</td>\n      <td>0.685590</td>\n      <td>f1</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n      <td>...</td>\n      <td>{'use_orig_features': True, 'max_base_models': 25, 'max_base_models_per_type': 5, 'save_bag_folds': True}</td>\n      <td>{}</td>\n      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': None, 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None, 'drop_unique': False}</td>\n      <td>[fnlwgt, workclass, education-num, sex, race, occupation, capital-loss, hours-per-week, relationship, native-country, marital-status, education, capital-gain, age]</td>\n      <td>None</td>\n      <td>{'learning_rate': 0.05, 'num_boost_round': 200}</td>\n      <td>{'num_boost_round': 83}</td>\n      <td>{'max_memory_usage_ratio': 1.0, 'max_time_limit_ratio': 1.0, 'max_time_limit': None, 'min_time_limit': 0, 'valid_raw_types': ['bool', 'int', 'float', 'category'], 'valid_special_types': None, 'ignored_type_group_special': None, 'ignored_type_group_raw': None, 'get_features_kwargs': None, 'get_features_kwargs_extra': None, 'predict_1_batch_size': None, 'temperature_scalar': None}</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 32 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"predictor.leaderboard(test_data, extra_metrics=['accuracy', 'balanced_accuracy', 'log_loss'])","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:13.012018Z","iopub.execute_input":"2024-07-06T06:05:13.012514Z","iopub.status.idle":"2024-07-06T06:05:15.558623Z","shell.execute_reply.started":"2024-07-06T06:05:13.012474Z","shell.execute_reply":"2024-07-06T06:05:15.557460Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                    model  score_test  accuracy  balanced_accuracy  log_loss  \\\n0  NeuralNetFastAI_BAG_L1    0.648383  0.846453           0.760403 -0.381144   \n1     WeightedEnsemble_L2    0.648383  0.846453           0.760403 -0.381144   \n2         LightGBM_BAG_L1    0.629437  0.847170           0.743784 -0.334022   \n\n   score_val eval_metric  pred_time_test  pred_time_val   fit_time  \\\n0   0.689243          f1        1.936818       0.316566  26.157116   \n1   0.689243          f1        1.939293       0.321463  26.357361   \n2   0.685590          f1        0.492885       0.238575  15.994304   \n\n   pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  \\\n0                 1.936818                0.316566          26.157116   \n1                 0.002475                0.004896           0.200245   \n2                 0.492885                0.238575          15.994304   \n\n   stack_level  can_infer  fit_order  \n0            1       True          2  \n1            2       True          3  \n2            1       True          1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_test</th>\n      <th>accuracy</th>\n      <th>balanced_accuracy</th>\n      <th>log_loss</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_test</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_test_marginal</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NeuralNetFastAI_BAG_L1</td>\n      <td>0.648383</td>\n      <td>0.846453</td>\n      <td>0.760403</td>\n      <td>-0.381144</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>1.936818</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>1.936818</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>0.648383</td>\n      <td>0.846453</td>\n      <td>0.760403</td>\n      <td>-0.381144</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>1.939293</td>\n      <td>0.321463</td>\n      <td>26.357361</td>\n      <td>0.002475</td>\n      <td>0.004896</td>\n      <td>0.200245</td>\n      <td>2</td>\n      <td>True</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LightGBM_BAG_L1</td>\n      <td>0.629437</td>\n      <td>0.847170</td>\n      <td>0.743784</td>\n      <td>-0.334022</td>\n      <td>0.685590</td>\n      <td>f1</td>\n      <td>0.492885</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>0.492885</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"i = 0  # index of model to use\nmodel_to_use = predictor.model_names()[i]\nmodel_pred = predictor.predict(datapoint, model=model_to_use)\nprint(\"Prediction from %s model: %s\" % (model_to_use, model_pred.iloc[0]))","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:15.560207Z","iopub.execute_input":"2024-07-06T06:05:15.560571Z","iopub.status.idle":"2024-07-06T06:05:15.647391Z","shell.execute_reply.started":"2024-07-06T06:05:15.560542Z","shell.execute_reply":"2024-07-06T06:05:15.646418Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Prediction from LightGBM_BAG_L1 model:  <=50K\n","output_type":"stream"}]},{"cell_type":"code","source":"all_models = predictor.model_names()\nmodel_to_use = all_models[i]\nspecific_model = predictor._trainer.load_model(model_to_use)\n\n# Objects defined below are dicts of various information (not printed here as they are quite large):\nmodel_info = specific_model.get_info()\npredictor_information = predictor.info()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:15.649071Z","iopub.execute_input":"2024-07-06T06:05:15.649692Z","iopub.status.idle":"2024-07-06T06:05:22.788162Z","shell.execute_reply.started":"2024-07-06T06:05:15.649651Z","shell.execute_reply":"2024-07-06T06:05:22.787193Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"y_pred_proba = predictor.predict_proba(test_data_nolabel)\nperf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred_proba)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:22.789743Z","iopub.execute_input":"2024-07-06T06:05:22.790218Z","iopub.status.idle":"2024-07-06T06:05:24.861719Z","shell.execute_reply.started":"2024-07-06T06:05:22.790178Z","shell.execute_reply":"2024-07-06T06:05:24.860673Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"perf = predictor.evaluate(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:24.863183Z","iopub.execute_input":"2024-07-06T06:05:24.863539Z","iopub.status.idle":"2024-07-06T06:05:26.753193Z","shell.execute_reply.started":"2024-07-06T06:05:24.863509Z","shell.execute_reply":"2024-07-06T06:05:26.751925Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"predictor.feature_importance(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:05:26.754674Z","iopub.execute_input":"2024-07-06T06:05:26.755155Z","iopub.status.idle":"2024-07-06T06:06:34.151429Z","shell.execute_reply.started":"2024-07-06T06:05:26.755112Z","shell.execute_reply":"2024-07-06T06:06:34.150135Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Computing feature importance via permutation shuffling for 14 features using 5000 rows with 5 shuffle sets...\n\t77.31s\t= Expected runtime (15.46s per shuffle set)\n\t67.36s\t= Actual runtime (Completed 5 of 5 shuffle sets)\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"                importance    stddev       p_value  n  p99_high   p99_low\neducation-num     0.091644  0.004709  8.333091e-07  5  0.101340  0.081949\nrelationship      0.063299  0.006310  1.169529e-05  5  0.076291  0.050306\nmarital-status    0.063249  0.001933  1.045302e-07  5  0.067229  0.059270\ncapital-gain      0.053084  0.005908  1.811530e-05  5  0.065250  0.040919\nage               0.035967  0.006231  1.038780e-04  5  0.048796  0.023138\noccupation        0.031620  0.006179  1.663760e-04  5  0.044342  0.018898\nhours-per-week    0.024741  0.005385  2.531423e-04  5  0.035829  0.013653\nworkclass         0.005685  0.006418  5.935034e-02  5  0.018901 -0.007530\ncapital-loss      0.005325  0.002638  5.355318e-03  5  0.010756 -0.000107\nsex               0.004770  0.003614  2.095723e-02  5  0.012210 -0.002671\neducation         0.004701  0.003312  1.686842e-02  5  0.011520 -0.002119\nnative-country    0.003612  0.003299  3.529318e-02  5  0.010404 -0.003181\nrace              0.002749  0.003211  6.404898e-02  5  0.009360 -0.003862\nfnlwgt            0.002481  0.004313  1.338472e-01  5  0.011361 -0.006399","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>importance</th>\n      <th>stddev</th>\n      <th>p_value</th>\n      <th>n</th>\n      <th>p99_high</th>\n      <th>p99_low</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>education-num</th>\n      <td>0.091644</td>\n      <td>0.004709</td>\n      <td>8.333091e-07</td>\n      <td>5</td>\n      <td>0.101340</td>\n      <td>0.081949</td>\n    </tr>\n    <tr>\n      <th>relationship</th>\n      <td>0.063299</td>\n      <td>0.006310</td>\n      <td>1.169529e-05</td>\n      <td>5</td>\n      <td>0.076291</td>\n      <td>0.050306</td>\n    </tr>\n    <tr>\n      <th>marital-status</th>\n      <td>0.063249</td>\n      <td>0.001933</td>\n      <td>1.045302e-07</td>\n      <td>5</td>\n      <td>0.067229</td>\n      <td>0.059270</td>\n    </tr>\n    <tr>\n      <th>capital-gain</th>\n      <td>0.053084</td>\n      <td>0.005908</td>\n      <td>1.811530e-05</td>\n      <td>5</td>\n      <td>0.065250</td>\n      <td>0.040919</td>\n    </tr>\n    <tr>\n      <th>age</th>\n      <td>0.035967</td>\n      <td>0.006231</td>\n      <td>1.038780e-04</td>\n      <td>5</td>\n      <td>0.048796</td>\n      <td>0.023138</td>\n    </tr>\n    <tr>\n      <th>occupation</th>\n      <td>0.031620</td>\n      <td>0.006179</td>\n      <td>1.663760e-04</td>\n      <td>5</td>\n      <td>0.044342</td>\n      <td>0.018898</td>\n    </tr>\n    <tr>\n      <th>hours-per-week</th>\n      <td>0.024741</td>\n      <td>0.005385</td>\n      <td>2.531423e-04</td>\n      <td>5</td>\n      <td>0.035829</td>\n      <td>0.013653</td>\n    </tr>\n    <tr>\n      <th>workclass</th>\n      <td>0.005685</td>\n      <td>0.006418</td>\n      <td>5.935034e-02</td>\n      <td>5</td>\n      <td>0.018901</td>\n      <td>-0.007530</td>\n    </tr>\n    <tr>\n      <th>capital-loss</th>\n      <td>0.005325</td>\n      <td>0.002638</td>\n      <td>5.355318e-03</td>\n      <td>5</td>\n      <td>0.010756</td>\n      <td>-0.000107</td>\n    </tr>\n    <tr>\n      <th>sex</th>\n      <td>0.004770</td>\n      <td>0.003614</td>\n      <td>2.095723e-02</td>\n      <td>5</td>\n      <td>0.012210</td>\n      <td>-0.002671</td>\n    </tr>\n    <tr>\n      <th>education</th>\n      <td>0.004701</td>\n      <td>0.003312</td>\n      <td>1.686842e-02</td>\n      <td>5</td>\n      <td>0.011520</td>\n      <td>-0.002119</td>\n    </tr>\n    <tr>\n      <th>native-country</th>\n      <td>0.003612</td>\n      <td>0.003299</td>\n      <td>3.529318e-02</td>\n      <td>5</td>\n      <td>0.010404</td>\n      <td>-0.003181</td>\n    </tr>\n    <tr>\n      <th>race</th>\n      <td>0.002749</td>\n      <td>0.003211</td>\n      <td>6.404898e-02</td>\n      <td>5</td>\n      <td>0.009360</td>\n      <td>-0.003862</td>\n    </tr>\n    <tr>\n      <th>fnlwgt</th>\n      <td>0.002481</td>\n      <td>0.004313</td>\n      <td>1.338472e-01</td>\n      <td>5</td>\n      <td>0.011361</td>\n      <td>-0.006399</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"predictor.persist()\n\nnum_test = 20\npreds = np.array(['']*num_test, dtype='object')\nfor i in range(num_test):\n    datapoint = test_data_nolabel.iloc[[i]]\n    pred_numpy = predictor.predict(datapoint, as_pandas=False)\n    preds[i] = pred_numpy[0]\n\nperf = predictor.evaluate_predictions(y_test[:num_test], preds, auxiliary_metrics=True)\nprint(\"Predictions: \", preds)\n\npredictor.unpersist()  # free memory by clearing models, future predict() calls will load models from disk","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:06:34.153217Z","iopub.execute_input":"2024-07-06T06:06:34.153660Z","iopub.status.idle":"2024-07-06T06:06:40.242446Z","shell.execute_reply.started":"2024-07-06T06:06:34.153621Z","shell.execute_reply":"2024-07-06T06:06:40.241437Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Persisting 2 models in memory. Models will require 0.0% of memory.\nUnpersisted 2 models: ['WeightedEnsemble_L2', 'NeuralNetFastAI_BAG_L1']\n","output_type":"stream"},{"name":"stdout","text":"Predictions:  [' <=50K' ' <=50K' ' >50K' ' <=50K' ' <=50K' ' >50K' ' >50K' ' >50K'\n ' <=50K' ' <=50K' ' <=50K' ' <=50K' ' <=50K' ' <=50K' ' <=50K' ' <=50K'\n ' <=50K' ' >50K' ' >50K' ' <=50K']\n","output_type":"stream"},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"['WeightedEnsemble_L2', 'NeuralNetFastAI_BAG_L1']"},"metadata":{}}]},{"cell_type":"code","source":"# At most 0.05 ms per row (20000 rows per second throughput)\ninfer_limit = 0.00005\n# adhere to infer_limit with batches of size 10000 (batch-inference, easier to satisfy infer_limit)\ninfer_limit_batch_size = 10000\n# adhere to infer_limit with batches of size 1 (online-inference, much harder to satisfy infer_limit)\n# infer_limit_batch_size = 1  # Note that infer_limit<0.02 when infer_limit_batch_size=1 can be difficult to satisfy.\npredictor_infer_limit = TabularPredictor(label=label, eval_metric=metric).fit(\n    train_data=train_data,\n    time_limit=30,\n    infer_limit=infer_limit,\n    infer_limit_batch_size=infer_limit_batch_size,\n)\n\n# NOTE: If bagging was enabled, it is important to call refit_full at this stage.\n#  infer_limit assumes that the user will call refit_full after fit.\n# predictor_infer_limit.refit_full()\n\n# NOTE: To align with inference speed calculated during fit, models must be persisted.\npredictor_infer_limit.persist()\n# Below is an optimized version that only persists the minimum required models for prediction.\n# predictor_infer_limit.persist('best')\n\npredictor_infer_limit.leaderboard()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:06:40.244114Z","iopub.execute_input":"2024-07-06T06:06:40.244916Z","iopub.status.idle":"2024-07-06T06:07:04.615472Z","shell.execute_reply.started":"2024-07-06T06:06:40.244874Z","shell.execute_reply":"2024-07-06T06:07:04.614119Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20240706_060640\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\nCPU Count:          4\nMemory Avail:       29.14 GB / 31.36 GB (92.9%)\nDisk Space Avail:   19.47 GB / 19.52 GB (99.8%)\n===================================================\nNo presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\nBeginning AutoGluon training ... Time limit = 30s\nAutoGluon will save models to \"AutogluonModels/ag-20240706_060640\"\nTrain Data Rows:    1000\nTrain Data Columns: 14\nLabel Column:       class\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\t2 unique label values:  [' >50K', ' <=50K']\n\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    29843.88 MB\n\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n\t\t('int', ['bool']) : 1 | ['sex']\n\t0.1s = Fit runtime\n\t14 features in original data used to generate 14 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\n\t3.216μs\t= Feature Preprocessing Time (1 row | 10000 batch size)\n\t\tFeature Preprocessing requires 6.43% of the overall inference constraint (0.05ms)\n\t\t0.047ms inference time budget remaining for models...\nData preprocessing and feature engineering runtime = 0.23s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': {},\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n\t'CAT': {},\n\t'XGB': {},\n\t'FASTAI': {},\n\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n}\nFitting 13 L1 models ...\nFitting model: KNeighborsUnif ... Training model for up to 29.77s of the 29.77s of remaining time.\n\t0.725\t = Validation score   (accuracy)\n\t0.79s\t = Training   runtime\n\t0.0s\t = Validation runtime\n\t6.687μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t6.687μs\t = Validation runtime (1 row | 10000 batch size)\n\t6.687μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t6.687μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: KNeighborsDist ... Training model for up to 28.96s of the 28.95s of remaining time.\n\t0.71\t = Validation score   (accuracy)\n\t0.07s\t = Training   runtime\n\t0.0s\t = Validation runtime\n\t5.856μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t5.856μs\t = Validation runtime (1 row | 10000 batch size)\n\t5.856μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t5.856μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: LightGBMXT ... Training model for up to 28.87s of the 28.87s of remaining time.\n\t0.85\t = Validation score   (accuracy)\n\t0.51s\t = Training   runtime\n\t0.01s\t = Validation runtime\n\t4.384μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t4.384μs\t = Validation runtime (1 row | 10000 batch size)\n\t4.384μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t4.384μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: LightGBM ... Training model for up to 28.34s of the 28.34s of remaining time.\n\t0.84\t = Validation score   (accuracy)\n\t0.59s\t = Training   runtime\n\t0.01s\t = Validation runtime\n\t2.858μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t2.858μs\t = Validation runtime (1 row | 10000 batch size)\n\t2.858μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t2.858μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: RandomForestGini ... Training model for up to 27.73s of the 27.72s of remaining time.\n\t0.84\t = Validation score   (accuracy)\n\t1.34s\t = Training   runtime\n\t0.11s\t = Validation runtime\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: RandomForestEntr ... Training model for up to 26.24s of the 26.24s of remaining time.\n\t0.835\t = Validation score   (accuracy)\n\t1.34s\t = Training   runtime\n\t0.1s\t = Validation runtime\n\t0.014ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t0.014ms\t = Validation runtime (1 row | 10000 batch size)\n\t0.014ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t0.014ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: CatBoost ... Training model for up to 24.77s of the 24.76s of remaining time.\n\t0.86\t = Validation score   (accuracy)\n\t5.01s\t = Training   runtime\n\t0.01s\t = Validation runtime\n\t1.733μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t1.733μs\t = Validation runtime (1 row | 10000 batch size)\n\t1.733μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t1.733μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: ExtraTreesGini ... Training model for up to 19.74s of the 19.73s of remaining time.\n\t0.815\t = Validation score   (accuracy)\n\t1.37s\t = Training   runtime\n\t0.1s\t = Validation runtime\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: ExtraTreesEntr ... Training model for up to 18.22s of the 18.22s of remaining time.\n\t0.82\t = Validation score   (accuracy)\n\t1.34s\t = Training   runtime\n\t0.11s\t = Validation runtime\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: NeuralNetFastAI ... Training model for up to 16.73s of the 16.73s of remaining time.\nNo improvement since epoch 7: early stopping\n\t0.84\t = Validation score   (accuracy)\n\t1.78s\t = Training   runtime\n\t0.02s\t = Validation runtime\n\t0.022ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t0.022ms\t = Validation runtime (1 row | 10000 batch size)\n\t0.022ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t0.022ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: XGBoost ... Training model for up to 14.91s of the 14.91s of remaining time.\n\t0.845\t = Validation score   (accuracy)\n\t0.45s\t = Training   runtime\n\t0.01s\t = Validation runtime\n\t3.868μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t3.868μs\t = Validation runtime (1 row | 10000 batch size)\n\t3.868μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t3.868μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: NeuralNetTorch ... Training model for up to 14.43s of the 14.43s of remaining time.\n\t0.85\t = Validation score   (accuracy)\n\t5.89s\t = Training   runtime\n\t0.02s\t = Validation runtime\n\t7.518μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t7.518μs\t = Validation runtime (1 row | 10000 batch size)\n\t7.518μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t7.518μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\nFitting model: LightGBMLarge ... Training model for up to 8.5s of the 8.5s of remaining time.\n\t0.815\t = Validation score   (accuracy)\n\t1.7s\t = Training   runtime\n\t0.01s\t = Validation runtime\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t0.015ms\t = Validation runtime (1 row | 10000 batch size | REFIT)\nRemoving 8/13 base models to satisfy inference constraint (constraint=0.044ms) ...\n\t0.13ms\t-> 0.124ms\t(KNeighborsDist)\n\t0.124ms\t-> 0.117ms\t(KNeighborsUnif)\n\t0.117ms\t-> 0.102ms\t(ExtraTreesGini)\n\t0.102ms\t-> 0.087ms\t(LightGBMLarge)\n\t0.087ms\t-> 0.072ms\t(ExtraTreesEntr)\n\t0.072ms\t-> 0.057ms\t(RandomForestEntr)\n\t0.057ms\t-> 0.054ms\t(LightGBM)\n\t0.054ms\t-> 0.04ms\t(RandomForestGini)\nFitting model: WeightedEnsemble_L2 ... Training model for up to 29.77s of the 6.7s of remaining time.\n\tEnsemble Weights: {'CatBoost': 1.0}\n\t0.86\t = Validation score   (accuracy)\n\t0.09s\t = Training   runtime\n\t0.0s\t = Validation runtime\n\t0.066μs\t = Validation runtime (1 row | 10000 batch size | MARGINAL)\n\t1.799μs\t = Validation runtime (1 row | 10000 batch size)\n\t0.066μs\t = Validation runtime (1 row | 10000 batch size | REFIT | MARGINAL)\n\t1.799μs\t = Validation runtime (1 row | 10000 batch size | REFIT)\nAutoGluon training complete, total runtime = 23.43s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 22528.8 rows/s (200 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240706_060640\")\nPersisting 2 models in memory. Models will require 0.0% of memory.\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                  model  score_val eval_metric  pred_time_val  fit_time  \\\n0              CatBoost      0.860    accuracy       0.007682  5.008136   \n1   WeightedEnsemble_L2      0.860    accuracy       0.008878  5.101268   \n2            LightGBMXT      0.850    accuracy       0.007315  0.506764   \n3        NeuralNetTorch      0.850    accuracy       0.020672  5.888182   \n4               XGBoost      0.845    accuracy       0.010657  0.451483   \n5              LightGBM      0.840    accuracy       0.006867  0.594885   \n6       NeuralNetFastAI      0.840    accuracy       0.017240  1.775822   \n7      RandomForestGini      0.840    accuracy       0.109062  1.339365   \n8      RandomForestEntr      0.835    accuracy       0.104640  1.335847   \n9        ExtraTreesEntr      0.820    accuracy       0.106614  1.342894   \n10        LightGBMLarge      0.815    accuracy       0.009752  1.695054   \n11       ExtraTreesGini      0.815    accuracy       0.104710  1.366213   \n12       KNeighborsUnif      0.725    accuracy       0.003518  0.794777   \n13       KNeighborsDist      0.710    accuracy       0.003456  0.070200   \n\n    pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n0                 0.007682           5.008136            1       True   \n1                 0.001195           0.093132            2       True   \n2                 0.007315           0.506764            1       True   \n3                 0.020672           5.888182            1       True   \n4                 0.010657           0.451483            1       True   \n5                 0.006867           0.594885            1       True   \n6                 0.017240           1.775822            1       True   \n7                 0.109062           1.339365            1       True   \n8                 0.104640           1.335847            1       True   \n9                 0.106614           1.342894            1       True   \n10                0.009752           1.695054            1       True   \n11                0.104710           1.366213            1       True   \n12                0.003518           0.794777            1       True   \n13                0.003456           0.070200            1       True   \n\n    fit_order  \n0           7  \n1          14  \n2           3  \n3          12  \n4          11  \n5           4  \n6          10  \n7           5  \n8           6  \n9           9  \n10         13  \n11          8  \n12          1  \n13          2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CatBoost</td>\n      <td>0.860</td>\n      <td>accuracy</td>\n      <td>0.007682</td>\n      <td>5.008136</td>\n      <td>0.007682</td>\n      <td>5.008136</td>\n      <td>1</td>\n      <td>True</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>0.860</td>\n      <td>accuracy</td>\n      <td>0.008878</td>\n      <td>5.101268</td>\n      <td>0.001195</td>\n      <td>0.093132</td>\n      <td>2</td>\n      <td>True</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LightGBMXT</td>\n      <td>0.850</td>\n      <td>accuracy</td>\n      <td>0.007315</td>\n      <td>0.506764</td>\n      <td>0.007315</td>\n      <td>0.506764</td>\n      <td>1</td>\n      <td>True</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NeuralNetTorch</td>\n      <td>0.850</td>\n      <td>accuracy</td>\n      <td>0.020672</td>\n      <td>5.888182</td>\n      <td>0.020672</td>\n      <td>5.888182</td>\n      <td>1</td>\n      <td>True</td>\n      <td>12</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>XGBoost</td>\n      <td>0.845</td>\n      <td>accuracy</td>\n      <td>0.010657</td>\n      <td>0.451483</td>\n      <td>0.010657</td>\n      <td>0.451483</td>\n      <td>1</td>\n      <td>True</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>LightGBM</td>\n      <td>0.840</td>\n      <td>accuracy</td>\n      <td>0.006867</td>\n      <td>0.594885</td>\n      <td>0.006867</td>\n      <td>0.594885</td>\n      <td>1</td>\n      <td>True</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NeuralNetFastAI</td>\n      <td>0.840</td>\n      <td>accuracy</td>\n      <td>0.017240</td>\n      <td>1.775822</td>\n      <td>0.017240</td>\n      <td>1.775822</td>\n      <td>1</td>\n      <td>True</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>RandomForestGini</td>\n      <td>0.840</td>\n      <td>accuracy</td>\n      <td>0.109062</td>\n      <td>1.339365</td>\n      <td>0.109062</td>\n      <td>1.339365</td>\n      <td>1</td>\n      <td>True</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>RandomForestEntr</td>\n      <td>0.835</td>\n      <td>accuracy</td>\n      <td>0.104640</td>\n      <td>1.335847</td>\n      <td>0.104640</td>\n      <td>1.335847</td>\n      <td>1</td>\n      <td>True</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ExtraTreesEntr</td>\n      <td>0.820</td>\n      <td>accuracy</td>\n      <td>0.106614</td>\n      <td>1.342894</td>\n      <td>0.106614</td>\n      <td>1.342894</td>\n      <td>1</td>\n      <td>True</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>LightGBMLarge</td>\n      <td>0.815</td>\n      <td>accuracy</td>\n      <td>0.009752</td>\n      <td>1.695054</td>\n      <td>0.009752</td>\n      <td>1.695054</td>\n      <td>1</td>\n      <td>True</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ExtraTreesGini</td>\n      <td>0.815</td>\n      <td>accuracy</td>\n      <td>0.104710</td>\n      <td>1.366213</td>\n      <td>0.104710</td>\n      <td>1.366213</td>\n      <td>1</td>\n      <td>True</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>KNeighborsUnif</td>\n      <td>0.725</td>\n      <td>accuracy</td>\n      <td>0.003518</td>\n      <td>0.794777</td>\n      <td>0.003518</td>\n      <td>0.794777</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>KNeighborsDist</td>\n      <td>0.710</td>\n      <td>accuracy</td>\n      <td>0.003456</td>\n      <td>0.070200</td>\n      <td>0.003456</td>\n      <td>0.070200</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_data_batch = test_data.sample(infer_limit_batch_size, replace=True, ignore_index=True)\n\nimport time\ntime_start = time.time()\npredictor_infer_limit.predict(test_data_batch)\ntime_end = time.time()\n\ninfer_time_per_row = (time_end - time_start) / len(test_data_batch)\nrows_per_second = 1 / infer_time_per_row\ninfer_time_per_row_ratio = infer_time_per_row / infer_limit\nis_constraint_satisfied = infer_time_per_row_ratio <= 1\n\nprint(f'Model is able to predict {round(rows_per_second, 1)} rows per second. (User-specified Throughput = {1 / infer_limit})')\nprint(f'Model uses {round(infer_time_per_row_ratio * 100, 1)}% of infer_limit time per row.')\nprint(f'Model satisfies inference constraint: {is_constraint_satisfied}')","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:07:04.617663Z","iopub.execute_input":"2024-07-06T06:07:04.619223Z","iopub.status.idle":"2024-07-06T06:07:04.699742Z","shell.execute_reply.started":"2024-07-06T06:07:04.619168Z","shell.execute_reply":"2024-07-06T06:07:04.698342Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Model is able to predict 148821.8 rows per second. (User-specified Throughput = 20000.0)\nModel uses 13.4% of infer_limit time per row.\nModel satisfies inference constraint: True\n","output_type":"stream"}]},{"cell_type":"code","source":"additional_ensembles = predictor.fit_weighted_ensemble(expand_pareto_frontier=True)\nprint(\"Alternative ensembles you can use for prediction:\", additional_ensembles)\n\npredictor.leaderboard(only_pareto_frontier=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:07:04.701814Z","iopub.execute_input":"2024-07-06T06:07:04.702298Z","iopub.status.idle":"2024-07-06T06:07:04.903432Z","shell.execute_reply.started":"2024-07-06T06:07:04.702259Z","shell.execute_reply":"2024-07-06T06:07:04.902281Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Fitting model: WeightedEnsemble_L2Best ...\n\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 1.0}\n\t0.6892\t = Validation score   (f1)\n\t0.15s\t = Training   runtime\n\t0.0s\t = Validation runtime\n","output_type":"stream"},{"name":"stdout","text":"Alternative ensembles you can use for prediction: ['WeightedEnsemble_L2Best']\n","output_type":"stream"},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"                    model  score_val eval_metric  pred_time_val   fit_time  \\\n0  NeuralNetFastAI_BAG_L1   0.689243          f1       0.316566  26.157116   \n1         LightGBM_BAG_L1   0.685590          f1       0.238575  15.994304   \n\n   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n0                0.316566          26.157116            1       True   \n1                0.238575          15.994304            1       True   \n\n   fit_order  \n0          2  \n1          1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NeuralNetFastAI_BAG_L1</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>LightGBM_BAG_L1</td>\n      <td>0.685590</td>\n      <td>f1</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"model_for_prediction = additional_ensembles[0]\npredictions = predictor.predict(test_data, model=model_for_prediction)\npredictor.delete_models(models_to_delete=additional_ensembles, dry_run=False)  # delete these extra models so they don't affect rest of tutorial","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:07:04.908733Z","iopub.execute_input":"2024-07-06T06:07:04.909166Z","iopub.status.idle":"2024-07-06T06:07:06.853362Z","shell.execute_reply.started":"2024-07-06T06:07:04.909133Z","shell.execute_reply":"2024-07-06T06:07:06.852345Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Deleting model WeightedEnsemble_L2Best. All files under agModels-predictClass/models/WeightedEnsemble_L2Best will be removed.\n","output_type":"stream"}]},{"cell_type":"code","source":"refit_model_map = predictor.refit_full()\nprint(\"Name of each refit-full model corresponding to a previous bagged ensemble:\")\nprint(refit_model_map)\npredictor.leaderboard(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:07:06.854859Z","iopub.execute_input":"2024-07-06T06:07:06.855557Z","iopub.status.idle":"2024-07-06T06:07:10.878917Z","shell.execute_reply.started":"2024-07-06T06:07:06.855512Z","shell.execute_reply":"2024-07-06T06:07:10.877647Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"Refitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBM_BAG_L1_FULL ...\n\t0.38s\t = Training   runtime\nFitting 1 L1 models ...\nFitting model: NeuralNetFastAI_BAG_L1_FULL ...\n\tStopping at the best epoch learned earlier - 10.\n\t0.73s\t = Training   runtime\nFitting model: WeightedEnsemble_L2_FULL | Skipping fit via cloning parent ...\n\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 1.0}\n\t0.2s\t = Training   runtime\nUpdated best model to \"WeightedEnsemble_L2_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"WeightedEnsemble_L2_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 1.26s ... Best model: \"WeightedEnsemble_L2_FULL\"\n","output_type":"stream"},{"name":"stdout","text":"Name of each refit-full model corresponding to a previous bagged ensemble:\n{'LightGBM_BAG_L1': 'LightGBM_BAG_L1_FULL', 'NeuralNetFastAI_BAG_L1': 'NeuralNetFastAI_BAG_L1_FULL', 'WeightedEnsemble_L2': 'WeightedEnsemble_L2_FULL'}\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"                         model  score_test  score_val eval_metric  \\\n0       NeuralNetFastAI_BAG_L1    0.648383   0.689243          f1   \n1          WeightedEnsemble_L2    0.648383   0.689243          f1   \n2         LightGBM_BAG_L1_FULL    0.634860        NaN          f1   \n3              LightGBM_BAG_L1    0.629437   0.685590          f1   \n4  NeuralNetFastAI_BAG_L1_FULL    0.494355        NaN          f1   \n5     WeightedEnsemble_L2_FULL    0.494355        NaN          f1   \n\n   pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  \\\n0        1.905391       0.316566  26.157116                 1.905391   \n1        1.907988       0.321463  26.357361                 0.002597   \n2        0.056253            NaN   0.380998                 0.056253   \n3        0.441928       0.238575  15.994304                 0.441928   \n4        0.239385            NaN   0.730678                 0.239385   \n5        0.241983            NaN   0.930923                 0.002598   \n\n   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n0                0.316566          26.157116            1       True   \n1                0.004896           0.200245            2       True   \n2                     NaN           0.380998            1       True   \n3                0.238575          15.994304            1       True   \n4                     NaN           0.730678            1       True   \n5                     NaN           0.200245            2       True   \n\n   fit_order  \n0          2  \n1          3  \n2          4  \n3          1  \n4          5  \n5          6  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_test</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_test</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_test_marginal</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NeuralNetFastAI_BAG_L1</td>\n      <td>0.648383</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>1.905391</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>1.905391</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>0.648383</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>1.907988</td>\n      <td>0.321463</td>\n      <td>26.357361</td>\n      <td>0.002597</td>\n      <td>0.004896</td>\n      <td>0.200245</td>\n      <td>2</td>\n      <td>True</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LightGBM_BAG_L1_FULL</td>\n      <td>0.634860</td>\n      <td>NaN</td>\n      <td>f1</td>\n      <td>0.056253</td>\n      <td>NaN</td>\n      <td>0.380998</td>\n      <td>0.056253</td>\n      <td>NaN</td>\n      <td>0.380998</td>\n      <td>1</td>\n      <td>True</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LightGBM_BAG_L1</td>\n      <td>0.629437</td>\n      <td>0.685590</td>\n      <td>f1</td>\n      <td>0.441928</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>0.441928</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NeuralNetFastAI_BAG_L1_FULL</td>\n      <td>0.494355</td>\n      <td>NaN</td>\n      <td>f1</td>\n      <td>0.239385</td>\n      <td>NaN</td>\n      <td>0.730678</td>\n      <td>0.239385</td>\n      <td>NaN</td>\n      <td>0.730678</td>\n      <td>1</td>\n      <td>True</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>WeightedEnsemble_L2_FULL</td>\n      <td>0.494355</td>\n      <td>NaN</td>\n      <td>f1</td>\n      <td>0.241983</td>\n      <td>NaN</td>\n      <td>0.930923</td>\n      <td>0.002598</td>\n      <td>NaN</td>\n      <td>0.200245</td>\n      <td>2</td>\n      <td>True</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"student_models = predictor.distill(time_limit=30)  # specify much longer time limit in real applications\nprint(student_models)\npreds_student = predictor.predict(test_data_nolabel, model=student_models[0])\nprint(f\"predictions from {student_models[0]}:\", list(preds_student)[:5])\npredictor.leaderboard(test_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:09:01.235402Z","iopub.execute_input":"2024-07-06T06:09:01.235923Z","iopub.status.idle":"2024-07-06T06:09:27.154619Z","shell.execute_reply.started":"2024-07-06T06:09:01.235887Z","shell.execute_reply":"2024-07-06T06:09:27.153084Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"Distilling with teacher='WeightedEnsemble_L2_FULL', teacher_preds=soft, augment_method=spunge ...\nSPUNGE: Augmenting training data with 4000 synthetic samples for distillation...\nDistilling with each of these student models: ['LightGBM_DSTL', 'CatBoost_DSTL', 'RandomForestMSE_DSTL', 'NeuralNetTorch_DSTL']\nFitting 4 L1 models ...\nFitting model: LightGBM_DSTL ... Training model for up to 30.0s of the 29.99s of remaining time.\n\tWarning: Exception caused LightGBM_DSTL to fail during training... Skipping this model.\n\t\tpandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: workclass: object, education: object, marital-status: object, occupation: object, relationship: object, race: object, native-country: object\nDetailed Traceback:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n    out = self._fit(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 218, in _fit\n    self.model = train_lgb_model(early_stopping_callback_kwargs=early_stopping_callback_kwargs, **train_params)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/lgb/lgb_utils.py\", line 128, in train_lgb_model\n    return lgb.train(**train_params)\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py\", line 255, in train\n    booster = Booster(params=params, train_set=train_set)\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 3433, in __init__\n    train_set.construct()\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 2462, in construct\n    self._lazy_init(data=self.data, label=self.label, reference=None,\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 2022, in _lazy_init\n    data, feature_name, categorical_feature, self.pandas_categorical = _data_from_pandas(\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 825, in _data_from_pandas\n    _pandas_to_numpy(data, target_dtype=target_dtype),\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 771, in _pandas_to_numpy\n    _check_for_bad_pandas_dtypes(data.dtypes)\n  File \"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py\", line 763, in _check_for_bad_pandas_dtypes\n    raise ValueError('pandas dtypes must be int, float or bool.\\n'\nValueError: pandas dtypes must be int, float or bool.\nFields with bad pandas dtypes: workclass: object, education: object, marital-status: object, occupation: object, relationship: object, race: object, native-country: object\nFitting model: CatBoost_DSTL ... Training model for up to 29.37s of the 29.36s of remaining time.\n\tWarning: Exception caused CatBoost_DSTL to fail during training... Skipping this model.\n\t\tfeatures data: pandas.DataFrame column 'workclass' has dtype 'category' but is not in  cat_features list\nDetailed Traceback:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n    out = self._fit(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 125, in _fit\n    X_val = Pool(data=X_val, label=y_val, cat_features=cat_features, weight=sample_weight_val)\n  File \"/opt/conda/lib/python3.10/site-packages/catboost/core.py\", line 848, in __init__\n    self._init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n  File \"/opt/conda/lib/python3.10/site-packages/catboost/core.py\", line 1481, in _init\n    self._init_pool(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n  File \"_catboost.pyx\", line 4159, in _catboost._PoolBase._init_pool\n  File \"_catboost.pyx\", line 4209, in _catboost._PoolBase._init_pool\n  File \"_catboost.pyx\", line 4025, in _catboost._PoolBase._init_features_order_layout_pool\n  File \"_catboost.pyx\", line 2919, in _catboost._set_features_order_data_pd_data_frame\n_catboost.CatBoostError: features data: pandas.DataFrame column 'workclass' has dtype 'category' but is not in  cat_features list\nFitting model: RandomForestMSE_DSTL ... Training model for up to 29.03s of the 29.03s of remaining time.\n/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/rf/rf_model.py:77: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  X = X.fillna(0).to_numpy(dtype=np.float32)\n\tNote: model has different eval_metric than default.\n\t-0.1213\t = Validation score   (-mean_squared_error)\n\t3.64s\t = Training   runtime\n\t0.1s\t = Validation runtime\nFitting model: NeuralNetTorch_DSTL ... Training model for up to 25.11s of the 25.1s of remaining time.\n\tWarning: Exception caused NeuralNetTorch_DSTL to fail during training... Skipping this model.\n\t\tFound array with 0 feature(s) (shape=(4800, 0)) while a minimum of 1 is required.\nDetailed Traceback:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1904, in _train_and_save\n    model = self._train_single(X, y, model, X_val, y_val, total_resources=total_resources, **model_fit_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/trainer/abstract_trainer.py\", line 1844, in _train_single\n    model = model.fit(X=X, y=y, X_val=X_val, y_val=y_val, total_resources=total_resources, **model_fit_kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n    out = self._fit(**kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 185, in _fit\n    train_dataset, val_dataset = self._generate_datasets(X=X, y=y, params=processor_kwargs, X_val=X_val, y_val=y_val)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 465, in _generate_datasets\n    train_dataset = self._process_train_data(\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/torch/tabular_nn_torch.py\", line 538, in _process_train_data\n    df = self.processor.fit_transform(df)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 273, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 914, in fit_transform\n    result = self._call_func_on_transformers(\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/compose/_column_transformer.py\", line 823, in _call_func_on_transformers\n    return Parallel(n_jobs=self.n_jobs)(jobs)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 67, in __call__\n    return super().__call__(iterable_with_config)\n  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 1918, in __call__\n    return output if self.return_generator else list(output)\n  File \"/opt/conda/lib/python3.10/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n    res = func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 129, in __call__\n    return self.function(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 1303, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1351, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/pipeline.py\", line 543, in fit_transform\n    return last_step.fit_transform(\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 273, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/base.py\", line 1061, in fit_transform\n    return self.fit(X, **fit_params).transform(X)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/utils/categorical_encoders.py\", line 727, in fit\n    self._fit(X, handle_unknown=\"ignore\")\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/utils/categorical_encoders.py\", line 193, in _fit\n    X_list, n_samples, n_features = self._check_X(X)\n  File \"/opt/conda/lib/python3.10/site-packages/autogluon/tabular/models/tabular_nn/utils/categorical_encoders.py\", line 164, in _check_X\n    X_temp = check_array(X, dtype=None, force_all_finite=False)\n  File \"/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 1035, in check_array\n    raise ValueError(\nValueError: Found array with 0 feature(s) (shape=(4800, 0)) while a minimum of 1 is required.\nRepeating k-fold bagging: 2/5\nRepeating k-fold bagging: 3/5\nRepeating k-fold bagging: 4/5\nRepeating k-fold bagging: 5/5\nCompleted 5/5 k-fold bagging repeats ...\nDistilling with each of these student models: ['WeightedEnsemble_L2_DSTL']\nFitting model: WeightedEnsemble_L2_DSTL ... Training model for up to 30.0s of the 24.71s of remaining time.\n\tEnsemble Weights: {'RandomForestMSE_DSTL': 1.0}\n\tNote: model has different eval_metric than default.\n\t-0.1213\t = Validation score   (-mean_squared_error)\n\t0.0s\t = Training   runtime\n\t0.0s\t = Validation runtime\nDistilled model leaderboard:\n                      model  score_val         eval_metric  pred_time_val  fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0      RandomForestMSE_DSTL   0.411765  mean_squared_error       0.100806  3.642198                0.100806           3.642198            1       True          7\n1  WeightedEnsemble_L2_DSTL   0.411765  mean_squared_error       0.101723  3.646958                0.000917           0.004760            2       True          8\n","output_type":"stream"},{"name":"stdout","text":"['RandomForestMSE_DSTL', 'WeightedEnsemble_L2_DSTL']\npredictions from RandomForestMSE_DSTL: [' <=50K', ' <=50K', ' <=50K', ' <=50K', ' <=50K']\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                         model  score_test  score_val         eval_metric  \\\n0       NeuralNetFastAI_BAG_L1    0.648383   0.689243                  f1   \n1          WeightedEnsemble_L2    0.648383   0.689243                  f1   \n2         LightGBM_BAG_L1_FULL    0.634860        NaN                  f1   \n3              LightGBM_BAG_L1    0.629437   0.685590                  f1   \n4         RandomForestMSE_DSTL    0.500607   0.411765  mean_squared_error   \n5     WeightedEnsemble_L2_DSTL    0.500607   0.411765  mean_squared_error   \n6  NeuralNetFastAI_BAG_L1_FULL    0.494355        NaN                  f1   \n7     WeightedEnsemble_L2_FULL    0.494355        NaN                  f1   \n\n   pred_time_test  pred_time_val   fit_time  pred_time_test_marginal  \\\n0        1.900910       0.316566  26.157116                 1.900910   \n1        1.903441       0.321463  26.357361                 0.002531   \n2        0.054709            NaN   0.380998                 0.054709   \n3        0.529927       0.238575  15.994304                 0.529927   \n4        0.342372       0.100806   3.642198                 0.342372   \n5        0.345657       0.101723   3.646958                 0.003284   \n6        0.224571            NaN   0.730678                 0.224571   \n7        0.227029            NaN   0.930923                 0.002459   \n\n   pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n0                0.316566          26.157116            1       True   \n1                0.004896           0.200245            2       True   \n2                     NaN           0.380998            1       True   \n3                0.238575          15.994304            1       True   \n4                0.100806           3.642198            1       True   \n5                0.000917           0.004760            2       True   \n6                     NaN           0.730678            1       True   \n7                     NaN           0.200245            2       True   \n\n   fit_order  \n0          2  \n1          3  \n2          4  \n3          1  \n4          7  \n5          8  \n6          5  \n7          6  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>model</th>\n      <th>score_test</th>\n      <th>score_val</th>\n      <th>eval_metric</th>\n      <th>pred_time_test</th>\n      <th>pred_time_val</th>\n      <th>fit_time</th>\n      <th>pred_time_test_marginal</th>\n      <th>pred_time_val_marginal</th>\n      <th>fit_time_marginal</th>\n      <th>stack_level</th>\n      <th>can_infer</th>\n      <th>fit_order</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NeuralNetFastAI_BAG_L1</td>\n      <td>0.648383</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>1.900910</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>1.900910</td>\n      <td>0.316566</td>\n      <td>26.157116</td>\n      <td>1</td>\n      <td>True</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>WeightedEnsemble_L2</td>\n      <td>0.648383</td>\n      <td>0.689243</td>\n      <td>f1</td>\n      <td>1.903441</td>\n      <td>0.321463</td>\n      <td>26.357361</td>\n      <td>0.002531</td>\n      <td>0.004896</td>\n      <td>0.200245</td>\n      <td>2</td>\n      <td>True</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>LightGBM_BAG_L1_FULL</td>\n      <td>0.634860</td>\n      <td>NaN</td>\n      <td>f1</td>\n      <td>0.054709</td>\n      <td>NaN</td>\n      <td>0.380998</td>\n      <td>0.054709</td>\n      <td>NaN</td>\n      <td>0.380998</td>\n      <td>1</td>\n      <td>True</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>LightGBM_BAG_L1</td>\n      <td>0.629437</td>\n      <td>0.685590</td>\n      <td>f1</td>\n      <td>0.529927</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>0.529927</td>\n      <td>0.238575</td>\n      <td>15.994304</td>\n      <td>1</td>\n      <td>True</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>RandomForestMSE_DSTL</td>\n      <td>0.500607</td>\n      <td>0.411765</td>\n      <td>mean_squared_error</td>\n      <td>0.342372</td>\n      <td>0.100806</td>\n      <td>3.642198</td>\n      <td>0.342372</td>\n      <td>0.100806</td>\n      <td>3.642198</td>\n      <td>1</td>\n      <td>True</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>WeightedEnsemble_L2_DSTL</td>\n      <td>0.500607</td>\n      <td>0.411765</td>\n      <td>mean_squared_error</td>\n      <td>0.345657</td>\n      <td>0.101723</td>\n      <td>3.646958</td>\n      <td>0.003284</td>\n      <td>0.000917</td>\n      <td>0.004760</td>\n      <td>2</td>\n      <td>True</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>NeuralNetFastAI_BAG_L1_FULL</td>\n      <td>0.494355</td>\n      <td>NaN</td>\n      <td>f1</td>\n      <td>0.224571</td>\n      <td>NaN</td>\n      <td>0.730678</td>\n      <td>0.224571</td>\n      <td>NaN</td>\n      <td>0.730678</td>\n      <td>1</td>\n      <td>True</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>WeightedEnsemble_L2_FULL</td>\n      <td>0.494355</td>\n      <td>NaN</td>\n      <td>f1</td>\n      <td>0.227029</td>\n      <td>NaN</td>\n      <td>0.930923</td>\n      <td>0.002459</td>\n      <td>NaN</td>\n      <td>0.200245</td>\n      <td>2</td>\n      <td>True</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"presets = ['good_quality', 'optimize_for_deployment']\npredictor_light = TabularPredictor(label=label, eval_metric=metric).fit(train_data, presets=presets, time_limit=30)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:10:30.903564Z","iopub.execute_input":"2024-07-06T06:10:30.904905Z","iopub.status.idle":"2024-07-06T06:11:11.022362Z","shell.execute_reply.started":"2024-07-06T06:10:30.904851Z","shell.execute_reply":"2024-07-06T06:11:11.021113Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20240706_061030\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\nCPU Count:          4\nMemory Avail:       28.97 GB / 31.36 GB (92.4%)\nDisk Space Avail:   19.33 GB / 19.52 GB (99.0%)\n===================================================\nPresets specified: ['good_quality', 'optimize_for_deployment']\nSetting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\nStack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\nNote: `save_bag_folds=False`! This will greatly reduce peak disk usage during fit (by ~8x), but runs the risk of an out-of-memory error during model refit if memory is small relative to the data size.\n\tYou can avoid this risk by setting `save_bag_folds=True`.\nDyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n\tRunning DyStack for up to 7s of the 30s of remaining time (25%).\n\t\tContext path: \"AutogluonModels/ag-20240706_061030/ds_sub_fit/sub_fit_ho\"\nLeaderboard on holdout data (DyStack):\n                      model  score_holdout  score_val eval_metric  pred_time_test pred_time_val  fit_time  pred_time_test_marginal pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n0    LightGBMXT_BAG_L1_FULL       0.866071   0.862613    accuracy        0.011224          None  1.391173                 0.011224                   None           1.391173            1       True          1\n1  WeightedEnsemble_L3_FULL       0.866071   0.862613    accuracy        0.013281          None  1.395271                 0.002057                   None           0.004098            3       True          3\n2  WeightedEnsemble_L2_FULL       0.866071   0.862613    accuracy        0.013518          None  1.395767                 0.002294                   None           0.004594            2       True          2\n\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n\t23s\t = DyStack   runtime |\t7s\t = Remaining runtime\nStarting main fit with num_stack_levels=1.\n\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\nBeginning AutoGluon training ... Time limit = 7s\nAutoGluon will save models to \"AutogluonModels/ag-20240706_061030\"\nTrain Data Rows:    1000\nTrain Data Columns: 14\nLabel Column:       class\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    29056.20 MB\n\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n\t\t('int', ['bool']) : 1 | ['sex']\n\t0.2s = Fit runtime\n\t14 features in original data used to generate 14 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.22s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\tTo change this, specify the eval_metric parameter of Predictor()\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': {},\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n\t'CAT': {},\n\t'XGB': {},\n\t'FASTAI': {},\n\t'RF': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n\t'XT': [{'criterion': 'gini', 'max_depth': 15, 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'max_depth': 15, 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'max_depth': 15, 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n}\nAutoGluon will fit 2 stack levels (L1 to L2) ...\nFitting 11 L1 models ...\nFitting model: LightGBMXT_BAG_L1 ... Training model for up to 4.41s of the 6.62s of remaining time.\n\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (4 workers, per: cpus=1, gpus=0, memory=0.01%)\n\t0.859\t = Validation score   (accuracy)\n\t11.36s\t = Training   runtime\n\t0.22s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 6.62s of the -9.51s of remaining time.\n\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n\t0.859\t = Validation score   (accuracy)\n\t0.01s\t = Training   runtime\n\t0.0s\t = Validation runtime\nFitting 11 L2 models ...\nFitting model: WeightedEnsemble_L3 ... Training model for up to 6.62s of the -9.57s of remaining time.\n\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n\t0.859\t = Validation score   (accuracy)\n\t0.01s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 16.44s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 563.0 rows/s (125 batch size)\nAutomatically performing refit_full as a post-fit operation (due to `.fit(..., refit_full=True)`\nRefitting models via `predictor.refit_full` using all of the data (combined train and validation)...\n\tModels trained in this way will have the suffix \"_FULL\" and have NaN validation score.\n\tThis process is not bound by time_limit, but should take less time than the original `predictor.fit` call.\n\tTo learn more, refer to the `.refit_full` method docstring which explains how \"_FULL\" models differ from normal models.\nFitting 1 L1 models ...\nFitting model: LightGBMXT_BAG_L1_FULL ...\n\t0.42s\t = Training   runtime\nUpdated best model to \"LightGBMXT_BAG_L1_FULL\" (Previously \"WeightedEnsemble_L2\"). AutoGluon will default to using \"LightGBMXT_BAG_L1_FULL\" for predict() and predict_proba().\nRefit complete, total runtime = 0.45s ... Best model: \"LightGBMXT_BAG_L1_FULL\"\nDeleting model LightGBMXT_BAG_L1. All files under AutogluonModels/ag-20240706_061030/models/LightGBMXT_BAG_L1 will be removed.\nDeleting model WeightedEnsemble_L2. All files under AutogluonModels/ag-20240706_061030/models/WeightedEnsemble_L2 will be removed.\nDeleting model WeightedEnsemble_L3. All files under AutogluonModels/ag-20240706_061030/models/WeightedEnsemble_L3 will be removed.\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240706_061030\")\n","output_type":"stream"}]},{"cell_type":"code","source":"predictor_light = TabularPredictor(label=label, eval_metric=metric).fit(train_data, hyperparameters='very_light', time_limit=30)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T06:11:11.024513Z","iopub.execute_input":"2024-07-06T06:11:11.025281Z","iopub.status.idle":"2024-07-06T06:11:25.247825Z","shell.execute_reply.started":"2024-07-06T06:11:11.025234Z","shell.execute_reply":"2024-07-06T06:11:25.246627Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"No path specified. Models will be saved in: \"AutogluonModels/ag-20240706_061111\"\nVerbosity: 2 (Standard Logging)\n=================== System Info ===================\nAutoGluon Version:  1.1.1\nPython Version:     3.10.13\nOperating System:   Linux\nPlatform Machine:   x86_64\nPlatform Version:   #1 SMP Thu Jun 27 20:43:36 UTC 2024\nCPU Count:          4\nMemory Avail:       28.53 GB / 31.36 GB (91.0%)\nDisk Space Avail:   19.33 GB / 19.52 GB (99.0%)\n===================================================\nNo presets specified! To achieve strong results with AutoGluon, it is recommended to use the available presets.\n\tRecommended Presets (For more details refer to https://auto.gluon.ai/stable/tutorials/tabular/tabular-essentials.html#presets):\n\tpresets='best_quality'   : Maximize accuracy. Default time_limit=3600.\n\tpresets='high_quality'   : Strong accuracy with fast inference speed. Default time_limit=3600.\n\tpresets='good_quality'   : Good accuracy with very fast inference speed. Default time_limit=3600.\n\tpresets='medium_quality' : Fast training time, ideal for initial prototyping.\nBeginning AutoGluon training ... Time limit = 30s\nAutoGluon will save models to \"AutogluonModels/ag-20240706_061111\"\nTrain Data Rows:    1000\nTrain Data Columns: 14\nLabel Column:       class\nAutoGluon infers your prediction problem is: 'binary' (because only two unique label-values observed).\n\t2 unique label values:  [' >50K', ' <=50K']\n\tIf 'binary' is not the correct problem_type, please manually specify the problem_type parameter during Predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression', 'quantile'])\nProblem Type:       binary\nPreprocessing data ...\nSelected class <--> label mapping:  class 1 =  >50K, class 0 =  <=50K\n\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive ( >50K) vs negative ( <=50K) class.\n\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\nUsing Feature Generators to preprocess the data ...\nFitting AutoMLPipelineFeatureGenerator...\n\tAvailable Memory:                    29211.16 MB\n\tTrain Data (Original)  Memory Usage: 0.56 MB (0.0% of available memory)\n\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n\tStage 1 Generators:\n\t\tFitting AsTypeFeatureGenerator...\n\t\t\tNote: Converting 1 features to boolean dtype as they only contain 2 unique values.\n\tStage 2 Generators:\n\t\tFitting FillNaFeatureGenerator...\n\tStage 3 Generators:\n\t\tFitting IdentityFeatureGenerator...\n\t\tFitting CategoryFeatureGenerator...\n\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n\tStage 4 Generators:\n\t\tFitting DropUniqueFeatureGenerator...\n\tStage 5 Generators:\n\t\tFitting DropDuplicatesFeatureGenerator...\n\tTypes of features in original data (raw dtype, special dtypes):\n\t\t('int', [])    : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n\t\t('object', []) : 8 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n\tTypes of features in processed data (raw dtype, special dtypes):\n\t\t('category', [])  : 7 | ['workclass', 'education', 'marital-status', 'occupation', 'relationship', ...]\n\t\t('int', [])       : 6 | ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', ...]\n\t\t('int', ['bool']) : 1 | ['sex']\n\t0.2s = Fit runtime\n\t14 features in original data used to generate 14 features in processed data.\n\tTrain Data (Processed) Memory Usage: 0.06 MB (0.0% of available memory)\nData preprocessing and feature engineering runtime = 0.2s ...\nAutoGluon will gauge predictive performance using evaluation metric: 'accuracy'\n\tTo change this, specify the eval_metric parameter of Predictor()\nAutomatically generating train/validation split with holdout_frac=0.2, Train Rows: 800, Val Rows: 200\nUser-specified model hyperparameters to be fit:\n{\n\t'NN_TORCH': {},\n\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n\t'CAT': {},\n\t'XGB': {},\n\t'FASTAI': {},\n}\nFitting 7 L1 models ...\nFitting model: LightGBMXT ... Training model for up to 29.8s of the 29.79s of remaining time.\n\t0.85\t = Validation score   (accuracy)\n\t0.52s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: LightGBM ... Training model for up to 29.25s of the 29.25s of remaining time.\n\t0.84\t = Validation score   (accuracy)\n\t0.65s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: CatBoost ... Training model for up to 28.58s of the 28.58s of remaining time.\n\t0.86\t = Validation score   (accuracy)\n\t4.22s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: NeuralNetFastAI ... Training model for up to 24.34s of the 24.34s of remaining time.\nNo improvement since epoch 7: early stopping\n\t0.84\t = Validation score   (accuracy)\n\t1.59s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitting model: XGBoost ... Training model for up to 22.7s of the 22.7s of remaining time.\n\t0.845\t = Validation score   (accuracy)\n\t0.36s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: NeuralNetTorch ... Training model for up to 22.31s of the 22.31s of remaining time.\n\t0.85\t = Validation score   (accuracy)\n\t4.52s\t = Training   runtime\n\t0.02s\t = Validation runtime\nFitting model: LightGBMLarge ... Training model for up to 17.77s of the 17.77s of remaining time.\n\t0.815\t = Validation score   (accuracy)\n\t1.71s\t = Training   runtime\n\t0.01s\t = Validation runtime\nFitting model: WeightedEnsemble_L2 ... Training model for up to 29.8s of the 15.97s of remaining time.\n\tEnsemble Weights: {'CatBoost': 1.0}\n\t0.86\t = Validation score   (accuracy)\n\t0.11s\t = Training   runtime\n\t0.0s\t = Validation runtime\nAutoGluon training complete, total runtime = 14.17s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 21982.1 rows/s (200 batch size)\nTabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240706_061111\")\n","output_type":"stream"}]}]}